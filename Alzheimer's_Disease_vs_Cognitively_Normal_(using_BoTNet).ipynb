{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FSCiZ8261d_p",
        "s_hotWvnL5Wt",
        "x_f9P7K1MHpX",
        "pSK3A4Q5QO_Q",
        "HrNpAbjVldfJ",
        "W6lnTEd8RvRg",
        "GvoJxUGHTfat",
        "pvKbS3H7vWmV",
        "NZ2vCpFvv0GA",
        "iUOmPtnqwMo2",
        "aSFMH4TjzEjx",
        "Rs-pXXkX1r91",
        "DuP7xttspgvG",
        "ZR_h5zms2TyY",
        "6X3SnUjy2jl5",
        "1cYHd_FL3KA2",
        "mMP0ZTOW3kdG",
        "bA3ijUmQ3tkI",
        "sJiqPC0v4UvY",
        "wLc_4g134hLw",
        "CX7ShXvS43ql",
        "qsn-67uN4702",
        "B_ku_w-M5Jdl",
        "Zw21RGKs5MrY",
        "CkKdlkr9GnXk",
        "ecYZ2iJTcySd"
      ],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1XMpcWOrjWm"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnsrEZRmroRK"
      },
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Uy1Rnzrzsz",
        "outputId": "ca2c808c-3d3e-4c78-ce32-04aa658305db"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  8 05:55:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSCiZ8261d_p"
      },
      "source": [
        "# Nitorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_hotWvnL5Wt"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEOS43G2LZFW"
      },
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp0JBcJPLkS2"
      },
      "source": [
        "def load_nifti(file_path, dtype=np.float32, incl_header=False, z_factor=None, mask=None):\n",
        "    \"\"\"\n",
        "    Loads a volumetric image in nifti format (extensions .nii, .nii.gz etc.)\n",
        "    as a 3D numpy.ndarray.\n",
        "\n",
        "    Args:\n",
        "        file_path: absolute path to the nifti file\n",
        "\n",
        "        dtype(optional): datatype of the loaded numpy.ndarray\n",
        "\n",
        "        incl_header(bool, optional): If True, the nifTI object of the\n",
        "        image is also returned.\n",
        "\n",
        "        z_factor(float or sequence, optional): The zoom factor along the\n",
        "        axes. If a float, zoom is the same for each axis. If a sequence,\n",
        "        zoom should contain one value for each axis.\n",
        "\n",
        "        mask(ndarray, optional): A mask with the same shape as the\n",
        "        original image. If provided then the mask is element-wise\n",
        "        multiplied with the image ndarray\n",
        "\n",
        "    Returns:\n",
        "        3D numpy.ndarray with axis order (saggital x coronal x axial)\n",
        "    \"\"\"\n",
        "\n",
        "    img = nib.load(file_path)\n",
        "    struct_arr = img.get_data().astype(dtype)\n",
        "\n",
        "    # replace infinite values with 0\n",
        "    if np.inf in struct_arr:\n",
        "        struct_arr[struct_arr == np.inf] = 0.\n",
        "\n",
        "    # replace NaN values with 0\n",
        "    if np.isnan(struct_arr).any() == True:\n",
        "        struct_arr[np.isnan(struct_arr)] = 0.\n",
        "\n",
        "    if mask is not None:\n",
        "        struct_arr *= mask\n",
        "\n",
        "    if z_factor is not None:\n",
        "        struct_arr = zoom(struct_arr, z_factor)\n",
        "\n",
        "    if incl_header:\n",
        "        return struct_arr, img\n",
        "    else:\n",
        "        return struct_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUAkRsGPLtWj"
      },
      "source": [
        "def show_brain(img, cut_coords=None,\n",
        "               figsize=(10,5), cmap=\"nipy_spectral\",\n",
        "               draw_cross = True,\n",
        "               return_fig = False\n",
        "               ):\n",
        "    \"\"\"Displays 2D cross-sections of a 3D image along all 3 axis\n",
        "    Arg:\n",
        "        img: can be (1) 3-dimensional numpy.ndarray\n",
        "                    (2) nibabel.Nifti1Image object\n",
        "                    (3) path to the image file stored in nifTI format\n",
        "        cut_coords (optional): The voxel coordinates\n",
        "        of the axes where the cross-section cuts will be performed.\n",
        "        Should be a 3-tuple: (x, y, z). Default is the center = img_shape/2\n",
        "\n",
        "        figsize (optional): matplotlib figsize. Default is (10,5)\n",
        "        cmap (optional): matplotlib colormap to be used\n",
        "\n",
        "        draw_cross (optional): Draws horizontal and vertical lines which\n",
        "        show where the cross-sections have been performed. D\n",
        "\n",
        "        example:\n",
        "            >>> show_brain(img, figsize=(7, 3), draw_cross=False)\n",
        "            >>> plt.show()\n",
        "        \"\"\"\n",
        "\n",
        "    if(isinstance(img, str) and os.path.isfile(img)):\n",
        "        img_arr = load_nifti(img)\n",
        "    elif(isinstance(img, nibabel.Nifti1Image)):\n",
        "        img_arr = img.get_data()\n",
        "\n",
        "    elif(isinstance(img, np.ndarray)):\n",
        "        assert img.ndim == 3, \"The numpy.ndarray must be 3-dimensional with shape (H x W x Z)\"\n",
        "        img_arr = img\n",
        "    else:\n",
        "        raise TypeError(\"Invalid type provided for 'img'- {}. \\\n",
        "Either provide a 3-dimensional numpy.ndarray of a MRI image or path to \\\n",
        "the image file stored as a nifTI format.\".format(type(img)))\n",
        "\n",
        "    # print(img_arr.shape)\n",
        "    # img_arr = np.moveaxis(img_arr, 0, 1)\n",
        "    # print(img_arr.shape)\n",
        "\n",
        "    x_len, y_len, z_len = img_arr.shape\n",
        "    # if cut_coordinates is not specified set it to the center of the image\n",
        "    if(cut_coords == None):\n",
        "        cut_coords = (x_len//2, y_len//2, z_len//2)\n",
        "\n",
        "    f, ax = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n",
        "\n",
        "    ax[0].set_title(\"Saggital cross-section at x={}\".format(cut_coords[0]))\n",
        "    ax[0].imshow(\n",
        "         np.rot90(img_arr[cut_coords[0],:,:]), cmap=cmap, aspect=\"equal\")\n",
        "    #draw cross\n",
        "    if(draw_cross):\n",
        "        ax[0].axvline(x=cut_coords[1], color='k', linewidth=1)\n",
        "        ax[0].axhline(y=cut_coords[2], color='k', linewidth=1)\n",
        "\n",
        "    ax[1].set_title(\"Coronal cross-section at y={}\".format(cut_coords[1]))\n",
        "    ax[1].imshow(\n",
        "        np.rot90(img_arr[:,cut_coords[1],:]), cmap=cmap, aspect=\"equal\")\n",
        "    ax[1].text(0.05, 0.95,'L',\n",
        "        horizontalalignment='left', verticalalignment='top',\n",
        "        transform=ax[1].transAxes\n",
        "        , bbox=dict(facecolor='white')\n",
        "        )\n",
        "    ax[1].text(0.95, 0.95,'R',\n",
        "        horizontalalignment='right', verticalalignment='top'\n",
        "        , transform=ax[1].transAxes\n",
        "        , bbox=dict(facecolor='white')\n",
        "        )\n",
        "    #draw cross\n",
        "    if(draw_cross):\n",
        "        ax[1].axvline(x=cut_coords[0], color='k', linewidth=1)\n",
        "        ax[1].axhline(y=cut_coords[2], color='k', linewidth=1)\n",
        "\n",
        "    ax[2].set_title(\"Axial cross-section at z={}\".format(cut_coords[2]))\n",
        "    ax[2].imshow(\n",
        "        np.rot90(img_arr[:,:,cut_coords[2]]), cmap=cmap, aspect=\"equal\"\n",
        "        )\n",
        "    ax[2].text(0.05, 0.95,'L'\n",
        "        , horizontalalignment='left', verticalalignment='top'\n",
        "        , transform=ax[2].transAxes\n",
        "        , bbox=dict(facecolor='white')\n",
        "        )\n",
        "    ax[2].text(0.95, 0.95,'R',\n",
        "        horizontalalignment='right', verticalalignment='top'\n",
        "        , transform=ax[2].transAxes\n",
        "        , bbox=dict(facecolor='white')\n",
        "        )\n",
        "    #draw cross\n",
        "    if(draw_cross):\n",
        "        ax[2].axvline(x=cut_coords[0], color='k', linewidth=1)\n",
        "        ax[2].axhline(y=cut_coords[1], color='k', linewidth=1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if return_fig:\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_f9P7K1MHpX"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3KsoD1APacy"
      },
      "source": [
        "import numbers\n",
        "import torch\n",
        "from scipy.ndimage.interpolation import rotate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qZPGHpCPbf8"
      },
      "source": [
        "def normalize_float(x, min=-1):\n",
        "    \"\"\"\n",
        "    Function that performs min-max normalization on a `numpy.ndarray`\n",
        "    matrix.\n",
        "    \"\"\"\n",
        "    if min == -1:\n",
        "        norm = (2 * (x - np.min(x)) / (np.max(x) - np.min(x))) - 1\n",
        "    elif min == 0:\n",
        "        if np.max(x) == 0 and np.min(x) == 0:\n",
        "            norm = x\n",
        "        else:\n",
        "            norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "    return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bew54Uv5Pksn"
      },
      "source": [
        "def normalize_float_torch(x, min=-1):\n",
        "    '''\n",
        "    Function that performs min-max normalization on a Pytorch tensor\n",
        "    matrix. Can also deal with Pytorch dictionaries where the data\n",
        "    matrix key is 'image'.\n",
        "    '''\n",
        "    import torch\n",
        "    if min == -1:\n",
        "        norm = (2 * (x - torch.min(x)) / (torch.max(x) - torch.min(x))) - 1\n",
        "    elif min == 0:\n",
        "        if torch.max(x) == 0 and torch.min(x) == 0:\n",
        "            norm = x\n",
        "        else:\n",
        "            norm = (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
        "    return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYC_K6HtPtw1"
      },
      "source": [
        "def normalization_factors(data, train_idx, shape, mode=\"slice\"):\n",
        "    \"\"\"\n",
        "    Shape should be of length 3.\n",
        "    mode : either \"slice\" or \"voxel\" - defines the granularity of the\n",
        "    normalization. Voxelwise normalization does not work well with only\n",
        "    linear registered data.\n",
        "    \"\"\"\n",
        "    print(\"Computing the normalization factors of the training data..\")\n",
        "    if mode == \"slice\":\n",
        "        axis = (0, 1, 2, 3)\n",
        "    elif mode == \"voxel\":\n",
        "        axis = 0\n",
        "    else:\n",
        "        raise NotImplementedError(\"Normalization mode unknown.\")\n",
        "    samples = np.zeros(\n",
        "        [len(train_idx), 1, shape[0], shape[1], shape[2]], dtype=np.float32\n",
        "    )\n",
        "    for c, value in enumerate(train_idx):\n",
        "        samples[c] = data[value][\"image\"].numpy()\n",
        "    mean = np.mean(samples, axis=axis)\n",
        "    std = np.std(samples, axis=axis)\n",
        "    return np.squeeze(mean), np.squeeze(std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aafxuu8lP28H"
      },
      "source": [
        "class CenterCrop(object):\n",
        "    \"\"\"Crops the given 3D ndarray Image at the center.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w, d), a cube crop (size, size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size), int(size))\n",
        "        else:\n",
        "            self.size = np.asarray(size)\n",
        "        assert len(self.size) == 3, \"The `size` must be a tuple of length 3 but is \\\n",
        "length {}\".format(len(self.size))\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            3D ndarray Image : Image to be cropped.\n",
        "        Returns:\n",
        "            3D ndarray Image: Cropped image.\n",
        "        \"\"\"\n",
        "        # if the 4th dimension of the image is the batch then ignore that dim\n",
        "        if len(img.shape) == 4:\n",
        "            img_size = img.shape[1:]\n",
        "        elif len(img.shape) == 3:\n",
        "            img_size = img.shape\n",
        "        else:\n",
        "            raise ValueError(\"The size of the image can be either 3 dimension or 4\\\n",
        "dimension with one dimension as the batch size\")\n",
        "\n",
        "        # crop only if the size of the image is bigger than the size to be cropped to.\n",
        "        if all(img_size >= self.size):\n",
        "            slice_start = (img_size - self.size)//2\n",
        "            slice_end = self.size + slice_start\n",
        "            cropped = img[slice_start[0]:slice_end[0],\n",
        "                          slice_start[1]:slice_end[1],\n",
        "                          slice_start[2]:slice_end[2]\n",
        "                         ]\n",
        "            if len(img.shape) == 4:\n",
        "                cropped = np.expand_dims(cropped, 0)\n",
        "        else:\n",
        "            cropped = img\n",
        "\n",
        "        return cropped\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4evWLLZQCtc"
      },
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"\n",
        "    Normalize tensor with first and second moments.\n",
        "    By default will only normalize on non-zero voxels. Set\n",
        "    masked = False if this is undesired.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std=1, masked=True, eps=1e-10):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.masked = masked\n",
        "        # set epsilon only if using std scaling\n",
        "        self.eps = eps if np.all(std) != 1 else 0\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.masked:\n",
        "            image = self.zero_masked_transform(image)\n",
        "        else:\n",
        "            image = self.apply_transform(image)\n",
        "        return image\n",
        "\n",
        "    def denormalize(self, image):\n",
        "        image = image * (self.std + self.eps) + self.mean\n",
        "        return image\n",
        "\n",
        "    def apply_transform(self, image):\n",
        "        return (image - self.mean) / (self.std + self.eps)\n",
        "\n",
        "    def zero_masked_transform(self, image):\n",
        "        \"\"\" Only apply transform where input is not zero. \"\"\"\n",
        "        img_mask = image == 0\n",
        "        # do transform\n",
        "        image = self.apply_transform(image)\n",
        "        image[img_mask] = 0.\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4G8cgwbQIp6"
      },
      "source": [
        "class IntensityRescale:\n",
        "    \"\"\"\n",
        "    Rescale image itensities between 0 and 1 for a single image.\n",
        "    Arguments:\n",
        "        masked: applies normalization only on non-zero voxels. Default\n",
        "            is True.\n",
        "        on_gpu: speed up computation by using GPU. Requires torch.Tensor\n",
        "             instead of np.array. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, masked=True, on_gpu=False):\n",
        "        self.masked = masked\n",
        "        self.on_gpu = on_gpu\n",
        "\n",
        "    def __call__(self, image):\n",
        "        if self.masked:\n",
        "            image = self.zero_masked_transform(image)\n",
        "        else:\n",
        "            image = self.apply_transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def apply_transform(self, image):\n",
        "        if self.on_gpu:\n",
        "            return normalize_float_torch(image, min=0)\n",
        "        else:\n",
        "            return normalize_float(image, min=0)\n",
        "\n",
        "    def zero_masked_transform(self, image):\n",
        "        \"\"\" Only apply transform where input is not zero. \"\"\"\n",
        "        img_mask = image == 0\n",
        "        # do transform\n",
        "        image = self.apply_transform(image)\n",
        "        image[img_mask] = 0.\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSK3A4Q5QO_Q"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXUFV3UQRr5"
      },
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"\n",
        "    Convert ndarrays to Tensors.\n",
        "    Expands channel axis\n",
        "    # numpy image: H x W x Z\n",
        "    # torch image: C x H x W x Z\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = torch.from_numpy(image).unsqueeze(0)\n",
        "        image = image.float()\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKdqEpPXQWAB"
      },
      "source": [
        "class Flip:\n",
        "    \"\"\"\n",
        "    Flip the input along a given axis.\n",
        "    Arguments:\n",
        "        axis: axis to flip over. Default is 0\n",
        "        prob: probability to flip the image. Executes always when set to\n",
        "             1. Default is 0.5\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=0, prob=0.5):\n",
        "        self.axis = axis\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image):\n",
        "        rand = np.random.uniform()\n",
        "        if rand <= self.prob:\n",
        "            augmented = np.flip(image, axis=self.axis).copy()\n",
        "        else:\n",
        "            augmented = image\n",
        "        return augmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_h9-3v5QZ2w"
      },
      "source": [
        "class SagittalFlip(Flip):\n",
        "    \"\"\"\n",
        "    Flip image along the sagittal axis (x-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, prob=0.5):\n",
        "        super().__init__(axis=0, prob=prob)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        assert(len(image.shape) == 3)\n",
        "        return super().__call__(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nVNop9QduO"
      },
      "source": [
        "class CoronalFlip(Flip):\n",
        "    \"\"\"\n",
        "    Flip image along the coronal axis (y-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, prob=0.5):\n",
        "        super().__init__(axis=1, prob=prob)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        assert(len(image.shape) == 3)\n",
        "        return super().__call__(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay1OUbD4QhiM"
      },
      "source": [
        "class AxialFlip(Flip):\n",
        "    \"\"\"\n",
        "    Flip image along the axial axis (z-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, prob=0.5):\n",
        "        super().__init__(axis=2, prob=prob)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        assert(len(image.shape) == 3)\n",
        "        return super().__call__(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2SBDE5XQqVx"
      },
      "source": [
        "class Rotate:\n",
        "    \"\"\"\n",
        "    Rotate the input along a given axis.\n",
        "    Arguments:\n",
        "        axis: axis to rotate. Default is 0\n",
        "        deg: min and max rotation angles in degrees. Randomly rotates\n",
        "            within that range. Can be scalar, list or tuple. In case of\n",
        "            scalar it rotates between -abs(deg) and abs(deg). Default is\n",
        "            (-3, 3).\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=0, deg=(-3, 3)):\n",
        "        if axis == 0:\n",
        "            self.axes = (1, 0)\n",
        "        elif axis == 1:\n",
        "            self.axes = (2, 1)\n",
        "        elif axis == 2:\n",
        "            self.axes = (0, 2)\n",
        "\n",
        "        if isinstance(deg, tuple) or isinstance(deg, list):\n",
        "            assert(len(deg) == 2)\n",
        "            self.min_rot = np.min(deg)\n",
        "            self.max_rot = np.max(deg)\n",
        "        else:\n",
        "            self.min_rot = -int(abs(deg))\n",
        "            self.max_rot = int(abs(deg))\n",
        "\n",
        "    def __call__(self, image):\n",
        "        rand = np.random.randint(self.min_rot, self.max_rot + 1)\n",
        "        augmented = rotate(\n",
        "            image,\n",
        "            angle=rand,\n",
        "            axes=self.axes,\n",
        "            reshape=False\n",
        "            ).copy()\n",
        "        return augmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNeo4NBfRKgP"
      },
      "source": [
        "class SagittalRotate(Rotate):\n",
        "    \"\"\"\n",
        "    Rotate image's sagittal axis (x-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, deg=(-3, 3)):\n",
        "        super().__init__(axis=0, deg=deg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTWANh9uROz6"
      },
      "source": [
        "class CoronalRotate(Rotate):\n",
        "    \"\"\"\n",
        "    Rotate image's coronal axis (y-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, deg=(-3, 3)):\n",
        "        super().__init__(axis=1, deg=deg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVrWxsLLRSUb"
      },
      "source": [
        "class AxialRotate(Rotate):\n",
        "    \"\"\"\n",
        "    Rotate image's axial axis (z-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, deg=(-3, 3)):\n",
        "        super().__init__(axis=2, deg=deg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_odotEeReir"
      },
      "source": [
        "class Translate:\n",
        "    \"\"\"\n",
        "    Translate the input along a given axis.\n",
        "    Arguments:\n",
        "        axis: axis to rotate. Default is 0\n",
        "        dist: min and max translation distance in pixels. Randomly\n",
        "            translates within that range. Can be scalar, list or tuple.\n",
        "            In case of scalar it translates between -abs(dist) and\n",
        "            abs(dist). Default is (-3, 3).\n",
        "    \"\"\"\n",
        "    def __init__(self, axis=0, dist=(-3, 3)):\n",
        "        self.axis = axis\n",
        "\n",
        "        if isinstance(dist, tuple) or isinstance(dist, list):\n",
        "            assert(len(dist) == 2)\n",
        "            self.min_trans = np.min(dist)\n",
        "            self.max_trans = np.max(dist)\n",
        "        else:\n",
        "            self.min_trans = -int(abs(dist))\n",
        "            self.max_trans = int(abs(dist))\n",
        "\n",
        "    def __call__(self, image):\n",
        "        rand = np.random.randint(self.min_trans, self.max_trans + 1)\n",
        "        augmented = np.zeros_like(image)\n",
        "        if self.axis == 0:\n",
        "            if rand < 0:\n",
        "                augmented[-rand:, :] = image[:rand, :]\n",
        "            elif rand > 0:\n",
        "                augmented[:-rand, :] = image[rand:, :]\n",
        "            else:\n",
        "                augmented = image\n",
        "        elif self.axis == 1:\n",
        "            if rand < 0:\n",
        "                augmented[:,-rand:, :] = image[:,:rand, :]\n",
        "            elif rand > 0:\n",
        "                augmented[:,:-rand, :] = image[:,rand:, :]\n",
        "            else:\n",
        "                augmented = image\n",
        "        elif self.axis == 2:\n",
        "            if rand < 0:\n",
        "                augmented[:,:,-rand:] = image[:,:,:rand]\n",
        "            elif rand > 0:\n",
        "                augmented[:,:,:-rand] = image[:,:,rand:]\n",
        "            else:\n",
        "                augmented = image\n",
        "        return augmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XutIv9mmRkIi"
      },
      "source": [
        "class SagittalTranslate(Translate):\n",
        "    \"\"\"\n",
        "    Translate image along the sagittal axis (x-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, dist=(-3, 3)):\n",
        "        super().__init__(axis=0, dist=dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMFTvvs0RnrN"
      },
      "source": [
        "class CoronalTranslate(Translate):\n",
        "    \"\"\"\n",
        "    Translate image along the coronal axis (y-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, dist=(-3, 3)):\n",
        "        super().__init__(axis=1, dist=dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ABZug3RrLo"
      },
      "source": [
        "class AxialTranslate(Translate):\n",
        "    \"\"\"\n",
        "    Translate image along the axial axis (z-axis).\n",
        "    Expects input shape (X, Y, Z).\n",
        "    \"\"\"\n",
        "    def __init__(self, dist=(-3, 3)):\n",
        "        super().__init__(axis=2, dist=dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNpAbjVldfJ"
      },
      "source": [
        "## Inferences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MurSdK7VldDa"
      },
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRjUEj7ellzy"
      },
      "source": [
        "def predict(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels,\n",
        "    prediction_type,\n",
        "    criterion,\n",
        "    **kwargs\n",
        "    ):\n",
        "    \"\"\" Predict according to loss and prediction type.\"\"\"\n",
        "    if prediction_type == \"binary\":\n",
        "        if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
        "            all_preds, all_labels = bce_with_logits_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels,\n",
        "                **kwargs\n",
        "            )\n",
        "        elif isinstance(criterion, nn.BCELoss):\n",
        "            all_preds, all_labels = bce_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels,\n",
        "                **kwargs\n",
        "            )\n",
        "    elif prediction_type == \"classification\":\n",
        "        all_preds, all_labels = crossentropy_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels\n",
        "        )\n",
        "    elif prediction_type == \"regression\":\n",
        "        # TODO: test different loss functions\n",
        "        all_preds, all_labels = regression_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels\n",
        "        )\n",
        "    elif prediction_type == \"reconstruction\":\n",
        "        # TODO: test different loss functions\n",
        "        all_preds, all_labels = regression_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels\n",
        "        )\n",
        "    elif prediction_type == \"variational\":\n",
        "        # TODO: test different loss functions\n",
        "        all_preds, all_labels = variational_inference(\n",
        "                outputs,\n",
        "                labels,\n",
        "                all_preds,\n",
        "                all_labels\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHBw07f2lvZF"
      },
      "source": [
        "def bce_with_logits_inference(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels,\n",
        "    **kwargs\n",
        "    ):\n",
        "    sigmoid = torch.sigmoid(outputs)\n",
        "    if kwargs[\"class_threshold\"]:\n",
        "        class_threshold = kwargs[\"class_threshold\"]\n",
        "    else:\n",
        "        class_threshold = 0.5\n",
        "    print\n",
        "    predicted = sigmoid.data >= class_threshold\n",
        "    for pred, label in zip(predicted, labels):\n",
        "        all_preds.append(pred.cpu().item())\n",
        "        all_labels.append(int(label.cpu().item()))\n",
        "    return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMHvDzREl4Ir"
      },
      "source": [
        "def bce_inference(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels,\n",
        "    **kwargs\n",
        "    ):\n",
        "    if kwargs[\"class_threshold\"]:\n",
        "        class_threshold = kwargs[\"class_threshold\"]\n",
        "    else:\n",
        "        class_threshold = 0.5\n",
        "    predicted = outputs.data >= class_threshold\n",
        "    for pred, label in zip(predicted, labels):\n",
        "        all_preds.append(pred.cpu().item())\n",
        "        all_labels.append(label.cpu().item())\n",
        "    return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7_ulhnRl8v_"
      },
      "source": [
        "def crossentropy_inference(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels,\n",
        "    **kwargs\n",
        "    ):\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    for pred, label in zip(predicted, labels):\n",
        "        all_preds.append(pred.cpu().item())\n",
        "        all_labels.append(label.cpu().item())\n",
        "    return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNrUgIMPmA7d"
      },
      "source": [
        "def regression_inference(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels\n",
        "    ):\n",
        "    # Multi-head case\n",
        "    # network returns a tuple of outputs\n",
        "    if isinstance(outputs, (list,tuple)):\n",
        "        predicted = [output.data for output in outputs]\n",
        "        for head in range(len(predicted)):\n",
        "            for j in range(len(predicted[head])):\n",
        "                try:\n",
        "                    all_preds[head].append(predicted[head][j].cpu().numpy()[0])\n",
        "                    all_labels[head].append(labels[head][j].cpu().numpy()[0])\n",
        "                except IndexError:\n",
        "                    # create inner lists if needed\n",
        "                    all_preds.append([predicted[head][j].cpu().numpy()[0]])\n",
        "                    all_labels.append([labels[head][j].cpu().numpy()[0]])\n",
        "        return all_preds, all_labels\n",
        "    # Single-head case\n",
        "    else:\n",
        "        predicted = outputs[0].data\n",
        "        # TODO: replace for loop with something faster\n",
        "        for j in range(len(predicted)):\n",
        "            try:\n",
        "                all_preds.append(predicted[j].cpu().numpy().item())\n",
        "                all_labels.append(labels[j].cpu().numpy().item())\n",
        "            except:\n",
        "                all_preds.append(predicted[j].cpu().numpy()[0])\n",
        "                all_labels.append(labels[j].cpu().numpy()[0])\n",
        "        return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQdbt1qamE3F"
      },
      "source": [
        "def variational_inference(\n",
        "    outputs,\n",
        "    labels,\n",
        "    all_preds,\n",
        "    all_labels\n",
        "    ):\n",
        "    \"\"\" Inference for variational autoencoders. \"\"\"\n",
        "    # VAE outputs reconstruction, mu and std\n",
        "    # select reconstruction only\n",
        "    outputs = outputs[0]\n",
        "    predicted = outputs.data\n",
        "    # TODO: replace for loop with something faster\n",
        "    for pred, label in zip(predicted, labels):\n",
        "        all_preds.append(pred.cpu().item())\n",
        "        all_labels.append(label.cpu().item())\n",
        "    return all_preds, all_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6lnTEd8RvRg"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPWHMxikTGqG"
      },
      "source": [
        "from matplotlib.lines import Line2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcewlPddTHtw"
      },
      "source": [
        "def dataset_length(data_loader):\n",
        "    \"\"\"\n",
        "    Return the full length of the dataset from the DataLoader alone.\n",
        "    Calling len(data_loader) only shows the number of mini-batches.\n",
        "    Requires data to be located at\n",
        "    \"\"\"\n",
        "    sample = next(iter(data_loader))\n",
        "\n",
        "    if isinstance(sample, dict):\n",
        "        try:\n",
        "            if isinstance(sample[\"label\"], torch.Tensor):\n",
        "                batch_size = sample[\"label\"].shape[0]\n",
        "            else:\n",
        "                # in case of sequence of inputs use first input\n",
        "                batch_size = sample[\"label\"][0].shape[0]\n",
        "        except:\n",
        "            KeyError(\"Expects key to be 'label'.\")\n",
        "    else:\n",
        "        if isinstance(sample[1], torch.Tensor):\n",
        "            batch_size = sample[1].shape[0]\n",
        "        else:\n",
        "            # in case of sequence of inputs use first input\n",
        "            batch_size = sample[1][0].shape[0]\n",
        "    return len(data_loader) * batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE78uaoATP9u"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA1pnso1TUmy"
      },
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "\n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90HcfRg7TaLl"
      },
      "source": [
        "def is_bad_grad(grad_output):\n",
        "    grad_output = grad_output.data\n",
        "    return grad_output.ne(grad_output).any() or grad_output.gt(1e6).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvoJxUGHTfat"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X_4yZQUUB4t"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQSCeZkUKQF"
      },
      "source": [
        "class _CAE_3D(nn.Module):\n",
        "    '''\n",
        "    Parent Convolutional Autoencoder class for 3D images.\n",
        "    All other Convolutional Autoencoder classes must inherit from this class.\n",
        "    '''\n",
        "    def __init__(self, conv_channels):\n",
        "        super().__init__()\n",
        "        # check if there are multiple convolution layers within a layer of the network or not\n",
        "        self.is_nested_conv = ([isinstance(each_c, (list, tuple)) for each_c in conv_channels])\n",
        "        if(any(self.is_nested_conv) and not all(self.is_nested_conv)):\n",
        "             raise TypeError(\" `conv_channels` can't be a mixture of both lists and ints.\")\n",
        "        self.is_nested_conv = any(self.is_nested_conv)\n",
        "\n",
        "        self.layers = len(conv_channels)\n",
        "        self.conv_channels = self._format_channels(conv_channels, self.is_nested_conv)\n",
        "        self.valid_activations = {'ELU': nn.ELU, 'HARDSHRINK': nn.Hardshrink, 'HARDTANH': nn.Hardtanh,\n",
        " 'LEAKYRELU':nn.LeakyReLU, 'LOGSIGMOID': nn.LogSigmoid, 'PRELU':nn.PReLU, 'RELU':nn.ReLU, 'RELU6': nn.ReLU6,\n",
        " 'RRELU': nn.RReLU, 'SELU': nn.SELU, 'SIGMOID': nn.Sigmoid, 'SOFTPLUS': nn.Softplus,\n",
        " 'SOFTSHRINK': nn.Softshrink, 'TANH': nn.Tanh, 'TANHSHRINK': nn.Tanhshrink, 'THRESHOLD': nn.Threshold}\n",
        "\n",
        "    def _format_channels(self, conv_channels, is_nested_conv = False):\n",
        "        channels = []\n",
        "        if(is_nested_conv):\n",
        "            for i in range(len(conv_channels)):\n",
        "                    inner_channels = []\n",
        "                    for j in range(len(conv_channels[i])):\n",
        "                        if (i == 0) and (j == 0):\n",
        "                            inner_channels.append([1, conv_channels[i][j]])\n",
        "                        elif (j == 0) :\n",
        "                            inner_channels.append([conv_channels[i-1][-1], conv_channels[i][j]])\n",
        "                        else:\n",
        "                            inner_channels.append([conv_channels[i][j-1], conv_channels[i][j]])\n",
        "                    channels.append(inner_channels)\n",
        "        else:\n",
        "            for i in range(len(conv_channels)):\n",
        "                if (i == 0):\n",
        "                    channels.append([1, conv_channels[i]])\n",
        "                else:\n",
        "                    channels.append([conv_channels[i-1], conv_channels[i]])\n",
        "\n",
        "        return channels\n",
        "\n",
        "\n",
        "    def assign_parameter(self, parameter, param_name, enable_nested = True):\n",
        "        ''' Wrapper for parameters of the Autoencoder.\n",
        "        Checks if the len and type of the parameter is acceptable.\n",
        "        If the parameter is just an single value,\n",
        "        makes its length equal to the number of layers defined in conv_channels\n",
        "        '''\n",
        "        if(isinstance(parameter, (int, str))):\n",
        "            if(self.is_nested_conv and enable_nested):\n",
        "                return_parameter = [len(inner_list)*[parameter] for inner_list in self.conv_channels]\n",
        "            else:\n",
        "                return_parameter = (self.layers * [parameter])\n",
        "        # Perform sanity checks if a list is already provided\n",
        "        elif(isinstance(parameter, (list, tuple))):\n",
        "            if(len(parameter) != self.layers):\n",
        "                raise ValueError(\"The parameter '{}' can either be a single int \\\n",
        "or must be a list of the same length as 'conv_channels'.\".format(\n",
        "        param_name))\n",
        "\n",
        "            if(self.is_nested_conv and enable_nested):\n",
        "                if(any(\n",
        "                    [len(c) != len(p) for c, p in zip(self.conv_channels, parameter)]\n",
        "                    )):\n",
        "                    raise ValueError(\"The lengths of the inner lists of the parameter {} \\\n",
        "have to be same as the 'conv_channels'\".format(param_name))\n",
        "            # if all length checks pass just return the parameter\n",
        "            return_parameter = parameter\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Parameter {} is neither an int/ valid str nor a list/tuple but is of type {}\".format(\n",
        "                param_name, parameter))\n",
        "\n",
        "        return return_parameter\n",
        "\n",
        "\n",
        "    def add_conv_with_activation(self, inp_channels, out_channels, kernel_size, padding, stride, activation_fn):\n",
        "        node = nn.Sequential(\n",
        "            nn.Conv3d(inp_channels, out_channels, kernel_size, padding = padding, stride = stride),\n",
        "            self.valid_activations[activation_fn](inplace=True))\n",
        "        return node\n",
        "\n",
        "\n",
        "    def add_deconv_with_activation(self, inp_channels, out_channels, kernel_size, padding, stride, out_padding, activation_fn):\n",
        "        node = nn.Sequential(\n",
        "            nn.ConvTranspose3d(inp_channels, out_channels, kernel_size\n",
        "                , padding = padding, stride = stride, output_padding=out_padding),\n",
        "            self.valid_activations[activation_fn](inplace=True))\n",
        "        return node\n",
        "\n",
        "\n",
        "    def add_pool(self, pool_type, kernel_size, padding, stride):\n",
        "        if(pool_type == \"max\"):\n",
        "            node = nn.MaxPool3d(kernel_size,\n",
        "                        padding = padding,\n",
        "                        stride = stride,\n",
        "                        return_indices = True)\n",
        "        elif(pool_type == \"avg\"):\n",
        "            node = nn.AvgPool3d(kernel_size,\n",
        "                        padding = padding,\n",
        "                        stride = stride)\n",
        "        else:\n",
        "            raise TypeError(\"Invalid value provided for `pool_type`.\\\n",
        "Allowed values are `max`, `avg`.\")\n",
        "\n",
        "        return node\n",
        "\n",
        "\n",
        "    def add_unpool(self, pool_type, kernel_size, padding, stride):\n",
        "        if(pool_type == \"max\"):\n",
        "            node = nn.MaxUnpool3d(kernel_size,\n",
        "                        padding = padding,\n",
        "                        stride = stride)\n",
        "        elif(pool_type == \"avg\"):\n",
        "            node = nn.MaxPool3d(kernel_size,\n",
        "                        padding = padding,\n",
        "                        stride = stride)\n",
        "        else:\n",
        "            raise TypeError(\"Invalid value provided for `pool_type`.\\\n",
        "Allowed values are `max`, `avg`.\")\n",
        "\n",
        "        return node\n",
        "\n",
        "\n",
        "    def nested_reverse(self, mylist):\n",
        "        result = []\n",
        "        for e in mylist:\n",
        "            if isinstance(e, (list, tuple)):\n",
        "                result.append(self.nested_reverse(e))\n",
        "            else:\n",
        "                result.append(e)\n",
        "        result.reverse()\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuXnVeQyUZJn"
      },
      "source": [
        "class CAE_3D(_CAE_3D):\n",
        "    '''\n",
        "    3D Convolutional Autoencoder model with only convolution layers. Strided convolution\n",
        "    can be used for undersampling.\n",
        "    '''\n",
        "    def __init__(self\n",
        "        , conv_channels\n",
        "        , activation_fn = \"RELU\"\n",
        "        , conv_kernel = 3\n",
        "        , conv_padding = 1\n",
        "        , conv_stride = 1\n",
        "        , deconv_out_padding = None\n",
        "        , second_fc_decoder = []\n",
        "        ):\n",
        "        '''\n",
        "        Args:\n",
        "            conv_channels : A list that defines the number of channels of each convolution layer.\n",
        "            The length of the list defines the number of layers in the encoder.\n",
        "            The decoder is automatically constructed as an exact reversal of the encoder architecture.\n",
        "\n",
        "            activation_fn (optional):  The non-linear activation function that will be appied after every layer\n",
        "            of convolution / deconvolution.\n",
        "            Supported values {'ELU', 'HARDSHRINK', 'HARDTANH', 'LEAKYRELU', 'LOGSIGMOID', 'PRELU', 'RELU',\n",
        "            'RELU6', 'RRELU', 'SELU', 'SIGMOID', 'SOFTPLUS', 'SOFTSHRINK', 'TANH', 'TANHSHRINK', 'THRESHOLD'}\n",
        "            By default nn.ReLu() is applied.\n",
        "            Can either be a a single int (in which case the same activation is applied to all layers) or\n",
        "            a list of same length and shape as `conv_channels`.\n",
        "\n",
        "            conv_kernel (optional): The size of the 3D convolutional kernels to be used.\n",
        "            Can either be a list of same length as `conv_channels` or a single int. In the\n",
        "             former case each value in the list represents the kernel size of that particular\n",
        "            layer and in the latter case all the layers are built with the same kernel size as\n",
        "            specified.\n",
        "            conv_padding (optional): The amount of zero-paddings to be done along each dimension.\n",
        "            Format same as conv_kernel.\n",
        "            conv_stride (optional): The stride of the 3D convolutions.\n",
        "            Format same as conv_kernel.\n",
        "            deconv_out_padding (optional): The additional zero-paddings to be done to the output\n",
        "            of ConvTranspose / Deconvolutions in the decoder network.\n",
        "            By default does (stride-1) number of padding.\n",
        "            Format same as conv_kernel.\n",
        "\n",
        "            second_fc_decoder (optional): By default this is disabled.\n",
        "            If a non-empty list of ints is provided then a secondary fully-connected decoder\n",
        "            network is constructed as per the list.\n",
        "            Each value represents the number of cells in each layer. Just like `conv_channels`\n",
        "            the length of the list defines the number of layers.\n",
        "            If enabled, the forward() method returns a list of 2 outputs, one from the Autoencoder's\n",
        "            decoder and the other from this fully-connected decoder network.\n",
        "        '''\n",
        "        super().__init__(conv_channels)\n",
        "\n",
        "        assert not(self.is_nested_conv), \"The conv_channels must be a list of ints (i.e. number of channels).\\\n",
        "It cannot be a list of lists.\"\n",
        "\n",
        "        self.conv_kernel = self.assign_parameter(conv_kernel, \"conv_kernel\")\n",
        "        self.conv_padding = self.assign_parameter(conv_padding, \"conv_kernel\")\n",
        "        self.conv_stride = self.assign_parameter(conv_stride, \"conv_stride\")\n",
        "        if(deconv_out_padding == None):\n",
        "            deconv_out_padding = [s-1 for s in self.conv_stride]\n",
        "        self.deconv_out_padding = self.assign_parameter(deconv_out_padding, \"deconv_out_padding\")\n",
        "\n",
        "        self.activation_fn = self.assign_parameter(activation_fn, \"activation_function\")\n",
        "\n",
        "        for activation in self.activation_fn:\n",
        "            assert activation.upper() in self.valid_activations.keys(), \"activation functions can only be one of the following str :\\n {}\".format(\n",
        "                    self.valid_activations.keys())\n",
        "\n",
        "        # set the switches used in forward() as false  by default\n",
        "        self.debug = False\n",
        "        self.return_encoder_out = False\n",
        "\n",
        "        if(second_fc_decoder):\n",
        "            self.second_fc_decoder = self._format_channels(second_fc_decoder)[1:]\n",
        "        else:\n",
        "            self.second_fc_decoder = []\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.deconvs = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.layers):\n",
        "            # build the encoder\n",
        "            self.convs.append(\n",
        "                self.add_conv_with_activation(\n",
        "                    self.conv_channels[i][0], self.conv_channels[i][1],\n",
        "                    self.conv_kernel[i]\n",
        "                    , self.conv_padding[i]\n",
        "                    , self.conv_stride[i]\n",
        "                    , self.activation_fn[i]\n",
        "                    )\n",
        "                )\n",
        "            # build the decoder\n",
        "            self.deconvs.append(\n",
        "                self.add_deconv_with_activation(\n",
        "                    self.conv_channels[-i-1][1], self.conv_channels[-i-1][0],\n",
        "                    self.conv_kernel[-i-1]\n",
        "                    , self.conv_padding[-i-1]\n",
        "                    , self.conv_stride[-i-1]\n",
        "                    , self.deconv_out_padding[-i-1]\n",
        "                    , self.activation_fn[-i-1]\n",
        "                    )\n",
        "                )\n",
        "        if(self.second_fc_decoder):\n",
        "        # build the second fc decoder\n",
        "            self.fcs = nn.ModuleList()\n",
        "            for layer in self.second_fc_decoder:\n",
        "                self.fcs.append(\n",
        "                    nn.Linear(layer[0], layer[1])\n",
        "                )\n",
        "\n",
        "\n",
        "    def set_debug(self, bool_val):\n",
        "        self.debug = bool_val\n",
        "\n",
        "    def set_return_encoder_out(self, bool_val):\n",
        "        self.return_encoder_out = bool_val\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "            if(self.debug): print(\"\\nImage dims =\"+str(x.size()))\n",
        "\n",
        "            #encoder\n",
        "            for i, conv in enumerate(self.convs):\n",
        "                x = conv(x)\n",
        "                if(self.debug): print(\"conv{} output dim = {}\".format(i+1, x.size()))\n",
        "\n",
        "            encoder_out = x\n",
        "\n",
        "            if(self.debug): print(\"\\nEncoder output dims =\"+str(encoder_out.size())+\"\\n\")\n",
        "\n",
        "            #decoder\n",
        "            for i, deconv in enumerate(self.deconvs):\n",
        "                x = deconv(x)\n",
        "                if(self.debug): print(\"deconv{} output dim = {}\".format(i+1, x.size()))\n",
        "\n",
        "            if(self.debug): print(\"\\nDecoder output dims =\"+str(x.size())+\"\\n\")\n",
        "\n",
        "            if(self.return_encoder_out):\n",
        "\n",
        "                return [x, encoder_out]\n",
        "            else:\n",
        "\n",
        "                return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwRZUNCCUhc_"
      },
      "source": [
        "class CAE_3D_with_pooling(_CAE_3D):\n",
        "    '''\n",
        "    3D Convolutional Autoencoder model with alternating Pooling layers.\n",
        "    '''\n",
        "    def __init__(self\n",
        "        , conv_channels\n",
        "        , activation_fn = nn.ReLU\n",
        "        , conv_kernel = 3, conv_padding = 1, conv_stride = 1\n",
        "        , pool_type = \"max\"\n",
        "        , pool_kernel = 2, pool_padding = 0, pool_stride = 2\n",
        "        , deconv_out_padding = None\n",
        "        ):\n",
        "        '''\n",
        "        Args:\n",
        "            conv_channels : A nested list whose length defines the number of layers. Each layer\n",
        "            can intern have multiple convolutions followed by a layer of Pooling. The lengths of the\n",
        "            inner list defines the number of convolutions per such layer and the value defines the number of\n",
        "            channels for each of these convolutions.\n",
        "            The decoder is constructed to be simply an exact reversal of the encoder architecture.\n",
        "\n",
        "            activation_fn (optional):  The non-linear activation function that will be appied after every layer\n",
        "            of convolution / deconvolution. By default nn.ReLu() is applied.\n",
        "            Supported values {'ELU', 'HARDSHRINK', 'HARDTANH', 'LEAKYRELU', 'LOGSIGMOID', 'PRELU', 'RELU',\n",
        "            'RELU6', 'RRELU', 'SELU', 'SIGMOID', 'SOFTPLUS', 'SOFTSHRINK', 'TANH', 'TANHSHRINK', 'THRESHOLD'}\n",
        "            Can either be a a single int (in which case the same activation is applied to all layers) or\n",
        "            a list of same length and shape as `conv_channels`.\n",
        "\n",
        "            conv_kernel (optional): The size of the 3D convolutional kernels to be used.\n",
        "            Can either be a list of lists of same lengths as `conv_channels` or a single int. In the\n",
        "             former case each value in the list represents the kernel size of that particular\n",
        "            layer and in the latter case all the layers are built with the same kernel size as\n",
        "            specified.\n",
        "            conv_padding (optional): The amount of zero-paddings to be done along each dimension.\n",
        "            Format same as conv_kernel.\n",
        "            conv_stride (optional): The stride of the 3D convolutions.\n",
        "            Format same as conv_kernel.\n",
        "            deconv_out_padding (optional): The additional zero-paddings to be done to the output\n",
        "            of ConvTranspose / Deconvolutions in the decoder network.\n",
        "            By default does (stride-1) number of padding.\n",
        "            Format same as conv_kernel.\n",
        "\n",
        "            pool_type (optional): The type of pooling to be used. Options are (1)\"max\"  (2)\"avg\"\n",
        "\n",
        "            pool_kernel, pool_padding, pool_stride (optional): Can either be a single int or a list\n",
        "            of respective pooling parameter values.\n",
        "            The length of these list must be same as length of conv_channels i.e. the number of layers.\n",
        "\n",
        "            second_fc_decoder (optional): By default this is disabled.\n",
        "            If a non-empty list of ints is provided then a secondary decoder of a fully-connected network\n",
        "            is constructed as per the list.\n",
        "        '''\n",
        "\n",
        "        super().__init__(conv_channels)\n",
        "\n",
        "        assert (self.is_nested_conv), \"The conv_channels must be a list of list of ints Ex. [[16],[32 64],[64],...] (i.e. number of channels).\\\n",
        "It cannot be a list.\"\n",
        "\n",
        "        self.conv_kernel = self.assign_parameter(conv_kernel, \"conv_kernel\")\n",
        "        self.conv_padding = self.assign_parameter(conv_padding, \"conv_padding\")\n",
        "        self.conv_stride = self.assign_parameter(conv_stride, \"conv_stride\")\n",
        "        self.pool_kernel = self.assign_parameter(pool_kernel, \"pool_kernel\", enable_nested=False)\n",
        "        self.pool_padding = self.assign_parameter(pool_padding, \"pool_padding\", enable_nested=False)\n",
        "        self.pool_stride = self.assign_parameter(pool_stride, \"pool_stride\", enable_nested=False)\n",
        "\n",
        "        self.activation_fn = self.assign_parameter(activation_fn, \"activation_function\")\n",
        "\n",
        "        for activations in self.activation_fn:\n",
        "            for activation in activations:\n",
        "                assert activation.upper() in self.valid_activations.keys(), \"activation functions can only be one of the following str :\\n {}\".format(\n",
        "                    self.valid_activations.keys())\n",
        "\n",
        "        self.deconv_channels = self.nested_reverse(self.conv_channels)\n",
        "        self.deconv_kernel = self.nested_reverse(self.conv_kernel)\n",
        "        self.deconv_padding = self.nested_reverse(self.conv_padding)\n",
        "        self.deconv_stride = self.nested_reverse(self.conv_stride)\n",
        "\n",
        "        # set the switches used by forward() as false by default\n",
        "        self.debug = False\n",
        "        self.return_encoder_out = False\n",
        "\n",
        "        if(deconv_out_padding is not None):\n",
        "            self.deconv_out_padding = self.nested_reverse(\n",
        "                self.assign_parameter(deconv_out_padding, \"deconv_out_padding\")\n",
        "            )\n",
        "        else:\n",
        "            self.deconv_out_padding = [[s-1 for s in layer] for layer in self.deconv_stride]\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.pools = nn.ModuleList()\n",
        "        self.deconvs = nn.ModuleList()\n",
        "        self.unpools = nn.ModuleList()\n",
        "\n",
        "\n",
        "        for i in range(self.layers):\n",
        "\n",
        "            self.convs.append(\n",
        "                nn.ModuleList(\n",
        "                    [self.add_conv_with_activation(\n",
        "                        inner_conv_channels[0], inner_conv_channels[1]\n",
        "                        , self.conv_kernel[i][j]\n",
        "                        , self.conv_padding[i][j]\n",
        "                        , self.conv_stride[i][j]\n",
        "                        , self.activation_fn[i][j]) \\\n",
        "                    for j, inner_conv_channels in enumerate(self.conv_channels[i])]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            self.deconvs.append(\n",
        "                nn.ModuleList(\n",
        "                    [self.add_deconv_with_activation(\n",
        "                        inner_deconv_channels[0], inner_deconv_channels[1]\n",
        "                        , self.deconv_kernel[i][j]\n",
        "                        , self.deconv_padding[i][j]\n",
        "                        , self.deconv_stride[i][j]\n",
        "                        , self.deconv_out_padding[i][j]\n",
        "                        , self.activation_fn[i][j]) \\\n",
        "                    for j, inner_deconv_channels in enumerate(self.deconv_channels[i])]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            self.pools.append(\n",
        "                self.add_pool(\n",
        "                    pool_type,\n",
        "                    self.pool_kernel[i],\n",
        "                    stride = self.pool_stride[i],\n",
        "                    padding = self.pool_padding[i]\n",
        "                )\n",
        "            )\n",
        "            self.unpools.append(\n",
        "                self.add_unpool(\n",
        "                    pool_type,\n",
        "                    self.pool_kernel[-i-1],\n",
        "                    stride = self.pool_stride[-i-1],\n",
        "                    padding = self.pool_padding[-i-1]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def set_debug(self, bool_val):\n",
        "        self.debug = bool_val\n",
        "\n",
        "    def set_return_encoder_out(self, bool_val):\n",
        "        self.return_encoder_out = bool_val\n",
        "\n",
        "    def forward(self, x):\n",
        "            '''return_encoder_out : If enabled returns a list with 2 values,\n",
        "            first one is the Autoencoder's output and the other the intermediary output of the encoder.\n",
        "            '''\n",
        "            pool_idxs = []\n",
        "            pool_sizes = [x.size()] #https://github.com/pytorch/pytorch/issues/580\n",
        "\n",
        "            if(self.debug):\n",
        "                print(\"\\nImage dims =\"+str(x.size()))\n",
        "\n",
        "            #encoder\n",
        "            for i,(convs, pool) in enumerate(zip(self.convs, self.pools)):\n",
        "                for j, conv in enumerate(convs):\n",
        "                    x = conv(x)\n",
        "                    if(self.debug):print(\"conv{}{} output dim = {}\".format(i+1, j+1, x.size()))\n",
        "\n",
        "                x, idx = pool(x)\n",
        "                pool_sizes.append(x.size())\n",
        "                pool_idxs.append(idx)\n",
        "                if(self.debug):print(\"pool{} output dim = {}\".format(i+1, x.size()))\n",
        "\n",
        "            encoder_out = x\n",
        "\n",
        "            if(self.debug):\n",
        "                print(\"\\nEncoder output dims =\"+str(encoder_out.size())+\"\\n\")\n",
        "\n",
        "            #decoder\n",
        "            pool_sizes.pop() # pop out the last size as it is not necessary\n",
        "\n",
        "            for i,(deconvs, unpool) in enumerate(zip(self.deconvs, self.unpools)):\n",
        "\n",
        "                x = unpool(x, pool_idxs.pop(), output_size=pool_sizes.pop())\n",
        "                if(self.debug):print(\"unpool{} output dim = {}\".format(i+1, x.size()))\n",
        "\n",
        "                for j, deconv in enumerate(deconvs):\n",
        "                    x = deconv(x)\n",
        "                    if(self.debug):print(\"deconv{}{} output dim = {}\".format(i+1, j+1, x.size()))\n",
        "\n",
        "            if(self.debug):\n",
        "                print(\"\\nDecoder output dims =\"+str(x.size())+\"\\n\")\n",
        "\n",
        "            if(self.return_encoder_out):\n",
        "                return [x, encoder_out]\n",
        "            else:\n",
        "                return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR5uAQcxUv7A"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    '''\n",
        "    Constructs fully-connected deep neural networks\n",
        "    '''\n",
        "    def __init__(self\n",
        "        , layers = []\n",
        "        , output_activation = nn.LogSoftmax\n",
        "        ):\n",
        "        '''\n",
        "        Args:\n",
        "            layer_neurons : Each value represents the number of neurons in each layer. The length of the list\n",
        "            defines the number of layers. '''\n",
        "        super().__init__()\n",
        "        self.layers = self._format_channels(layers)\n",
        "#         self.output_activation = output_activation\n",
        "        self.debug = False\n",
        "\n",
        "        # build the fully-connected layers\n",
        "        self.fcs = nn.ModuleList()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if(layer) is not self.layers[-1]:\n",
        "                self.fcs.append(self.add_linear_with_Relu(layer))\n",
        "            elif(output_activation is not None):\n",
        "                self.fcs.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.Linear(layer[0], layer[1]),\n",
        "                        output_activation()))\n",
        "            else:\n",
        "                self.fcs.append(\n",
        "                    nn.Linear(layer[0], layer[1]))\n",
        "\n",
        "    def set_debug(self, bool_val):\n",
        "        self.debug = bool_val\n",
        "\n",
        "    def _format_channels(self, layers):\n",
        "        layer_inout = []\n",
        "        for i in range(len(layers)-1):\n",
        "            layer_inout.append([layers[i], layers[i+1]])\n",
        "        return layer_inout\n",
        "\n",
        "    def add_linear_with_Relu(self, layer):\n",
        "        node = nn.Sequential(\n",
        "            nn.Linear(layer[0], layer[1]),\n",
        "            nn.ReLU(True))\n",
        "        return node\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i,fc in enumerate(self.fcs):\n",
        "            x = fc(x)\n",
        "            if(self.debug):print(\"FC {} output dims ={}\".format(i, x.size()))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvKbS3H7vWmV"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxRFLLUbvZKP"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLA1DeauvaPG"
      },
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scheduler=None,\n",
        "            metrics=[],\n",
        "            callbacks=[],\n",
        "            training_time_callback=None,\n",
        "            device=torch.device('cuda'),\n",
        "            prediction_type=\"binary\",\n",
        "            **kwargs\n",
        "    ):\n",
        "        \"\"\" Main class for training.\n",
        "        # Arguments\n",
        "            model: neural network to train.\n",
        "            criterion: loss function.\n",
        "            optimizer: optimization function.\n",
        "            scheduler: schedules the optimizer.\n",
        "            metrics: list of metrics to report. Default is None.\n",
        "            callbacks: list of callbacks to execute at the end of training epochs. Default is None.\n",
        "            training_time_callback: a user-defined callback that executes the model.forward()\n",
        "                and returns the output to the trainer.\n",
        "                This can be used to perform debug during train time, Visualize features,\n",
        "                call model.forward() with custom arguments, run multiple decoder networks etc.\n",
        "                Default is None.\n",
        "            class_threshold: classification threshold for binary\n",
        "                classification. Default is 0.5.\n",
        "            prediction_type: accepts one of [\"binary\", \"classification\",\n",
        "                \"regression\", \"reconstruction\", \"variational\", \"other\"].\n",
        "                This is used to determine output type.\n",
        "            device: The device to use for training. Must be integer or\n",
        "                    a torch.device object. By default, GPU with current\n",
        "                    node is used.\n",
        "        \"\"\"\n",
        "        if not isinstance(model, nn.Module):\n",
        "            raise ValueError(\"Expects model type to be torch.nn.Module\")\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.metrics = metrics\n",
        "        self.callbacks = callbacks\n",
        "        self.training_time_callback = training_time_callback\n",
        "        if isinstance(device, int):\n",
        "            self.device = torch.device(\"cuda:\" + str(device))\n",
        "        elif isinstance(device, torch.device):\n",
        "            self.device = device\n",
        "        else:\n",
        "            raise ValueError(\"Device needs to be of type torch.device or \\\n",
        "                integer.\")\n",
        "        if \"class_threshold\" in kwargs.keys():\n",
        "            self.class_threshold = kwargs[\"class_threshold\"]\n",
        "        else:\n",
        "            self.class_threshold = None\n",
        "        self.stop_training = False\n",
        "        self.start_time = None\n",
        "        self.prediction_type = prediction_type\n",
        "\n",
        "    def train_model(\n",
        "            self,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            inputs_key=\"image\",\n",
        "            labels_key=\"label\",\n",
        "            num_epochs=25,\n",
        "            show_train_steps=100,\n",
        "            show_validation_epochs=1\n",
        "    ):\n",
        "        \"\"\" Main function to train a network for one epoch.\n",
        "        Args:\n",
        "            train_loader: a pytorch Dataset iterator for training data\n",
        "            val_loader: a pytorch Dataset iterator for validation data\n",
        "            inputs_key, labels_key: The data returned by `train_loader` and `val_loader`can\n",
        "                            either be a dict of format data_loader[X_key] = inputs and\n",
        "                            data_loader[y_key] = labels or a list with data_loader[0] = inputs\n",
        "                            and data_loader[1] = labels. The default keys are \"image\" and \"label\".\n",
        "        \"\"\"\n",
        "        assert (show_validation_epochs < num_epochs) or (num_epochs == 1), \"\\\n",
        "'show_validation_epochs' value should be less than 'num_epochs'\"\n",
        "        assert (show_train_steps>0) and (show_train_steps<=len(train_loader)),\"\\\n",
        "'show_train_steps' value out of range. Must be > 0 and < len(train_loader)\"\n",
        "\n",
        "        val_metrics = dict()\n",
        "        train_metrics = dict()\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.best_metric = 0.0\n",
        "        self.best_model = None\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            if self.stop_training:\n",
        "                # TODO: check position of this\n",
        "                print(\"Early stopping in epoch {}\".format(epoch))\n",
        "                return self.finish_training(train_metrics, val_metrics, epoch)\n",
        "            else:\n",
        "                # running_loss accumulates loss every 'show_train_steps' cycles until it must be printed.\n",
        "                running_loss = np.array([])\n",
        "                epoch_loss = 0.0\n",
        "                if self.scheduler:\n",
        "                    self.scheduler.step(epoch)\n",
        "\n",
        "                # Reset all metrics related variables at the start of each epoch\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                self.multi_batch_metrics = dict()\n",
        "                # train mode\n",
        "                self.model.train()\n",
        "\n",
        "                for i, data in enumerate(train_loader):\n",
        "                    try:\n",
        "                        inputs, labels = data[inputs_key], data[labels_key]\n",
        "                    except TypeError:\n",
        "                        # if data does not come in dictionary, assume\n",
        "                        # that data is ordered like [input, label]\n",
        "                        try:\n",
        "                            inputs, labels = data[0], data[1]\n",
        "                        except TypeError:\n",
        "                            raise TypeError\n",
        "                    # in case of multi-input or output create a list\n",
        "                    if isinstance(inputs, list):\n",
        "                        inputs = [inp.to(self.device) for inp in inputs]\n",
        "                    else:\n",
        "                        inputs = inputs.to(self.device)\n",
        "                    if isinstance(labels, list):\n",
        "                        labels = [label.to(self.device) for label in labels]\n",
        "                    else:\n",
        "                        labels = labels.to(self.device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    self.optimizer.zero_grad()\n",
        "                    # forward + backward + optimize\n",
        "\n",
        "                    if self.training_time_callback is not None:\n",
        "                        outputs = self.training_time_callback(\n",
        "                            inputs,\n",
        "                            labels,\n",
        "                            i,\n",
        "                            epoch\n",
        "                        )\n",
        "                    else:\n",
        "                        outputs = self.model(inputs)\n",
        "\n",
        "                    if self.prediction_type == \"classification\":\n",
        "                        labels = labels.squeeze(1)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "\n",
        "                    # enable the below commented code if you want to visualize the\n",
        "                    # gradient flow through the model during training\n",
        "                    # plot_grad_flow(self.model.named_parameters())\n",
        "\n",
        "                    #self.optimizer.step()\n",
        "\n",
        "                    def closure():\n",
        "                      self.optimizer.zero_grad()\n",
        "                      outputs = self.model(inputs)\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      loss = self.criterion(outputs, labels)\n",
        "                      loss.backward()\n",
        "                      return loss\n",
        "\n",
        "                    self.optimizer.step(closure)\n",
        "\n",
        "                    # store results\n",
        "                    all_preds, all_labels = predict(\n",
        "                        outputs,\n",
        "                        labels,\n",
        "                        all_preds,\n",
        "                        all_labels,\n",
        "                        self.prediction_type,\n",
        "                        self.criterion,\n",
        "                        class_threshold=self.class_threshold\n",
        "                    )\n",
        "                    # update loss\n",
        "                    running_loss= np.append(running_loss, loss.item())\n",
        "                    epoch_loss += loss.item()\n",
        "                    # print loss every X mini-batches\n",
        "                    if (i % show_train_steps == 0) and (i != 0):\n",
        "                        print(\n",
        "                            \"[%d, %5d] loss: %.5f\"\n",
        "                            % (epoch , i ,\n",
        "                               running_loss.mean())\n",
        "                        )\n",
        "                        running_loss = np.array([]) #reset\n",
        "\n",
        "                    # compute training metrics for X/2 mini-batches\n",
        "                    # useful for large outputs (e.g. reconstructions)\n",
        "                    if self.prediction_type == \"reconstruction\":\n",
        "                        if i % int(show_train_steps/2) == 0:\n",
        "                            self.estimate_metrics(\n",
        "                                all_labels,\n",
        "                                all_preds,\n",
        "                            )\n",
        "                            # TODO: test if del helps\n",
        "                            all_labels = []\n",
        "                            all_preds = []\n",
        "\n",
        "                # report training metrics\n",
        "                # weighted averages of metrics are computed over batches\n",
        "                train_metrics = self._on_epoch_end(\n",
        "                        train_metrics,\n",
        "                        all_labels,\n",
        "                        all_preds,\n",
        "                        phase=\"train\"\n",
        "                    )\n",
        "                epoch_loss /= len(train_loader)\n",
        "\n",
        "                # add loss to metrics data\n",
        "                if \"loss\" in train_metrics:\n",
        "                    train_metrics[\"loss\"].append(epoch_loss)\n",
        "                else:\n",
        "                    train_metrics[\"loss\"] = [epoch_loss]\n",
        "\n",
        "                #<end-of-training-cycle-loop>\n",
        "            #<end-of-epoch-loop>\n",
        "\n",
        "            # validate every x iterations\n",
        "            if epoch % show_validation_epochs == 0:\n",
        "                self.model.eval()\n",
        "                validation_loss = 0.0\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                self.multi_batch_metrics = dict()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for i, data in enumerate(val_loader):\n",
        "                        try:\n",
        "                            inputs, labels = data[inputs_key], data[labels_key]\n",
        "                        except TypeError:\n",
        "                            # if data does not come in dictionary, assume\n",
        "                            # that data is ordered like [input, label]\n",
        "                            try:\n",
        "                                inputs, labels = data[0], data[1]\n",
        "                            except TypeError:\n",
        "                                raise TypeError(\"Data not in correct \\\n",
        "                                 sequence format.\")\n",
        "                        # in case of multi-input or output create a list\n",
        "                        if isinstance(inputs, list):\n",
        "                            inputs = [inp.to(self.device) for inp in inputs]\n",
        "                        else:\n",
        "                            inputs = inputs.to(self.device)\n",
        "                        if isinstance(labels, list):\n",
        "                            labels = [label.to(self.device) for label in labels]\n",
        "                        else:\n",
        "                            labels = labels.to(self.device)\n",
        "\n",
        "                        # forward pass only\n",
        "                        if self.training_time_callback is not None:\n",
        "                            outputs = self.training_time_callback(\n",
        "                                inputs,\n",
        "                                labels,\n",
        "                                1,  # dummy value\n",
        "                                1  # dummy value\n",
        "                            )\n",
        "                        else:\n",
        "                            outputs = self.model(inputs)\n",
        "\n",
        "                        if self.prediction_type == \"classification\":\n",
        "                            labels = labels.squeeze(1)\n",
        "                        loss = self.criterion(outputs, labels)\n",
        "                        # compute validation accuracy\n",
        "                        all_preds, all_labels = predict(\n",
        "                            outputs,\n",
        "                            labels,\n",
        "                            all_preds,\n",
        "                            all_labels,\n",
        "                            self.prediction_type,\n",
        "                            self.criterion,\n",
        "                            class_threshold=self.class_threshold\n",
        "                        )\n",
        "\n",
        "                        validation_loss += loss.item()\n",
        "\n",
        "                        # compute training metrics for X/2 mini-batches\n",
        "                        # useful for large outputs (e.g. reconstructions)\n",
        "                        if self.prediction_type == \"reconstruction\":\n",
        "                            if i % int(show_train_steps/2) == 0:\n",
        "                                self.estimate_metrics(\n",
        "                                    all_labels,\n",
        "                                    all_preds,\n",
        "                                )\n",
        "                                # TODO: test if del helps\n",
        "                                all_labels = []\n",
        "                                all_preds = []\n",
        "\n",
        "                    # report validation metrics\n",
        "                    # weighted averages of metrics are computed over batches\n",
        "                    val_metrics = self._on_epoch_end(\n",
        "                        val_metrics,\n",
        "                        all_labels,\n",
        "                        all_preds,\n",
        "                        phase=\"val\"\n",
        "                    )\n",
        "\n",
        "                    validation_loss /= len(val_loader)\n",
        "                    print(\"Val loss: {0:.6f}\".format(validation_loss))\n",
        "                    # add loss to metrics data\n",
        "                    if \"loss\" in val_metrics:\n",
        "                        val_metrics[\"loss\"].append(validation_loss)\n",
        "                    else:\n",
        "                        val_metrics[\"loss\"] = [validation_loss]\n",
        "            if self.callbacks is not None:\n",
        "                for callback in self.callbacks:\n",
        "                    callback(self, epoch, val_metrics)\n",
        "        # End training\n",
        "        return self.finish_training(train_metrics, val_metrics, epoch)\n",
        "\n",
        "    def finish_training(self, train_metrics, val_metrics, epoch):\n",
        "        \"\"\"\n",
        "        End the training cyle, return a model and finish callbacks.\n",
        "        \"\"\"\n",
        "        time_elapsed = int(time.time() - self.start_time)\n",
        "        print(\"Total time elapsed: {}h:{}m:{}s\".format(\n",
        "            time_elapsed // 3600, (time_elapsed // 60) % 60, time_elapsed % 60))\n",
        "        # execute final methods of callbacks\n",
        "        if self.callbacks is not None:\n",
        "            for callback in self.callbacks:\n",
        "                # find all methods of the callback\n",
        "                method_list = [\n",
        "                    func\n",
        "                    for func in dir(callback)\n",
        "                    if (callable(getattr(callback, func))\n",
        "                        and not func.startswith(\"__\"))\n",
        "                ]\n",
        "                if \"final\" in method_list:\n",
        "                    callback.final(trainer=self, epoch=epoch)\n",
        "        # in case of no model selection, pick the last loss\n",
        "        if self.best_metric == 0.0:\n",
        "            self.best_metric = val_metrics[\"loss\"][-1]\n",
        "            self.best_model = self.model\n",
        "\n",
        "        return (self.model,\n",
        "                {\n",
        "                    \"train_metrics\": train_metrics,\n",
        "                    \"val_metrics\": val_metrics,\n",
        "                    \"best_model\": self.best_model,\n",
        "                    \"best_metric\": self.best_metric}\n",
        "                )\n",
        "\n",
        "    def visualize_training(self, report, metrics=None, save_fig_path=\"\"):\n",
        "        # Plot loss first\n",
        "        plt.figure()\n",
        "        plt.plot(report[\"train_metrics\"][\"loss\"])\n",
        "        plt.plot(report[\"val_metrics\"][\"loss\"])\n",
        "        plt.title(\"Loss during training\")\n",
        "        plt.legend([\"Train\", \"Val\"])\n",
        "        if (save_fig_path):\n",
        "            plt.savefig(save_fig_path)\n",
        "        plt.show()\n",
        "        if metrics is None:\n",
        "            metrics = self.metrics\n",
        "        for metric in metrics:\n",
        "            plt.figure()\n",
        "            plt.plot(report[\"train_metrics\"][metric.__name__])\n",
        "            plt.plot(report[\"val_metrics\"][metric.__name__])\n",
        "            plt.legend([\"Train\", \"Val\"])\n",
        "            plt.title(metric.__name__)\n",
        "            if(save_fig_path):\n",
        "                plt.savefig(save_fig_path+\"_\"+metric.__name__)\n",
        "            plt.show()\n",
        "\n",
        "    def evaluate_model(\n",
        "            self,\n",
        "            val_loader,\n",
        "            additional_gpu=None,\n",
        "            metrics=None,\n",
        "            inputs_key=\"image\",\n",
        "            labels_key=\"label\"\n",
        "    ):\n",
        "        # predict on the validation set\n",
        "        \"\"\"\n",
        "        Predict on the validation set.\n",
        "        # Arguments\n",
        "            val_loader : data loader of the validation set\n",
        "            additional_gpu : GPU number if evaluation should be done on\n",
        "                separate GPU\n",
        "            metrics: list of\n",
        "        \"\"\"\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        if additional_gpu is not None:\n",
        "            device = additional_gpu\n",
        "        else:\n",
        "            device = self.device\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data[inputs_key], data[labels_key]\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                # run inference\n",
        "                all_preds, all_labels = predict(\n",
        "                    outputs,\n",
        "                    labels,\n",
        "                    all_preds,\n",
        "                    all_labels,\n",
        "                    self.prediction_type,\n",
        "                    self.criterion,\n",
        "                    class_threshold=self.class_threshold\n",
        "                )\n",
        "\n",
        "        # compute confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "\n",
        "        # Visualize the confusion matrix\n",
        "        classes = [\"control\", \"patient\"]\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "\n",
        "        fmt = \"d\"\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(\n",
        "                j,\n",
        "                i,\n",
        "                format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "            )\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.ylabel(\"True label\")\n",
        "        plt.xlabel(\"Predicted label\")\n",
        "        plt.show()\n",
        "\n",
        "        # print metrics\n",
        "        if metrics is not None:\n",
        "            for metric in metrics:\n",
        "                if isinstance(all_preds[0], list):\n",
        "                    print(\"{}: {}\".format(metric.__name__, np.mean([metric(labels, preds) for preds,labels in zip(all_preds, all_labels)])))\n",
        "                else:\n",
        "                    print(\"{}: {}\".format(metric.__name__, metric(all_labels, all_preds)))\n",
        "\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "    def report_metrics(\n",
        "        self,\n",
        "        metrics_dict,\n",
        "        phase\n",
        "        ):\n",
        "\n",
        "        # report execution time only in training phase\n",
        "        if (phase == \"train\"):\n",
        "            time_elapsed = int(time.time() - self.start_time)\n",
        "            print(\"Time elapsed: {}h:{}m:{}s\".format(\n",
        "                time_elapsed // 3600, (time_elapsed // 60) % 60, time_elapsed % 60))\n",
        "\n",
        "        \"\"\" Store and report a list of metric functions. \"\"\"\n",
        "        for metric in self.metrics:\n",
        "            # report everything but loss\n",
        "            if metric.__name__ is not \"loss\":\n",
        "                # weighted average over previous batches\n",
        "                # weigh by the number of samples per batch and divide by\n",
        "                # the total number of samples\n",
        "                batch_results = np.zeros(shape=(\n",
        "                    len(self.multi_batch_metrics[\"len_\" + metric.__name__])))\n",
        "                n_samples = 0\n",
        "                for b_idx, batch_len in enumerate(\n",
        "                    self.multi_batch_metrics[\"len_\" + metric.__name__]\n",
        "                    ):\n",
        "                    batch_results[b_idx] = self.multi_batch_metrics[\n",
        "                        metric.__name__][b_idx] * batch_len\n",
        "                    n_samples += batch_len\n",
        "\n",
        "                result = np.sum(batch_results) / n_samples\n",
        "\n",
        "                if metric.__name__ in metrics_dict:\n",
        "                    metrics_dict[metric.__name__].append(result)\n",
        "                else:\n",
        "                    metrics_dict[metric.__name__] = [result]\n",
        "                # print result\n",
        "                if isinstance(result, float):\n",
        "                    print(\"{} {}: {:.2f} %\".format(\n",
        "                        phase, metric.__name__, result * 100))\n",
        "                else:\n",
        "                    print(\"{} {}: {} \".format(\n",
        "                        phase, metric.__name__, str(result)))\n",
        "        return metrics_dict\n",
        "\n",
        "    def estimate_metrics(\n",
        "        self,\n",
        "        all_labels,\n",
        "        all_preds\n",
        "        ):\n",
        "        \"\"\" Estimate a list of metric functions. \"\"\"\n",
        "        n_predictions = len(all_preds)\n",
        "\n",
        "        for metric in self.metrics:\n",
        "            # report everything but loss\n",
        "            if metric.__name__ is not \"loss\":\n",
        "                if isinstance(all_preds[0], list):\n",
        "                    result = np.mean([metric(labels, preds) for preds,labels in zip(all_preds, all_labels)])\n",
        "                else:\n",
        "                    result = metric(all_labels, all_preds)\n",
        "\n",
        "                if metric.__name__ in self.multi_batch_metrics:\n",
        "                    self.multi_batch_metrics[metric.__name__].append(result)\n",
        "                    self.multi_batch_metrics[\"len_\" + metric.__name__].append(\n",
        "                        n_predictions)\n",
        "                else:\n",
        "                    self.multi_batch_metrics[metric.__name__] = [result]\n",
        "                    self.multi_batch_metrics[\"len_\" + metric.__name__] = [n_predictions]\n",
        "\n",
        "    def _on_epoch_end(\n",
        "        self,\n",
        "        metrics_dict,\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        phase\n",
        "        ):\n",
        "        # check for unreported metrics\n",
        "        if len(all_preds) > 0:\n",
        "            self.estimate_metrics(\n",
        "                    all_labels,\n",
        "                    all_preds,\n",
        "                )\n",
        "            # TODO: test if del helps\n",
        "            all_labels = []\n",
        "            all_preds = []\n",
        "\n",
        "        metrics_dict = self.report_metrics(\n",
        "            metrics_dict,\n",
        "            phase\n",
        "        )\n",
        "\n",
        "        return metrics_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ2vCpFvv0GA"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_DHlihjv64X"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50jTKCDov-ig"
      },
      "source": [
        "class BCE_KL_loss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Reconstruction loss for variational auto-encoders.\n",
        "    Binary-cross entropy reconstruction + KL divergence losses summed\n",
        "    over all elements and batch.\n",
        "    Mostly taken from pytorch examples:\n",
        "        https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "    Arguments:\n",
        "        outputs: List of the form [reconstruction, mean, logvariance].\n",
        "        x: ground-truth.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BCE_KL_loss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, target):\n",
        "        recon_x, mu, logvar = outputs\n",
        "        BCE = F.binary_cross_entropy(recon_x, target, size_average=False)\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSxDznIjwC5a"
      },
      "source": [
        "class MSE_KL_loss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Reconstruction loss for variational auto-encoders.\n",
        "    Mean squared error reconstruction + KL divergence losses summed\n",
        "    over all elements and batch.\n",
        "    Mostly taken from pytorch examples:\n",
        "        https://github.com/pytorch/examples/blob/master/vae/main.py\n",
        "    Arguments:\n",
        "        outputs: List of the form [reconstruction, mean, logvariance].\n",
        "        x: ground-truth.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MSE_KL_loss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, target):\n",
        "        recon_x, mu, logvar = outputs\n",
        "        MSE = F.mse_loss(recon_x, target, size_average=False)\n",
        "\n",
        "        # see Appendix B from VAE paper:\n",
        "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "        # https://arxiv.org/abs/1312.6114\n",
        "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return MSE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIkcqoM5wHR4"
      },
      "source": [
        "class Multihead_loss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Compute the loss on multiple outputs.\n",
        "    Arguments:\n",
        "        outputs: List of network outputs.\n",
        "        target: List of targets where len(outputs) = len(target).\n",
        "        loss_function: List of loss functions with either\n",
        "            len(loss_function) = len(targets) or len(loss_function) = 1.\n",
        "        weights: List of weights for each loss. Default = [1]\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_function, weights=[1]):\n",
        "        super(Multihead_loss, self).__init__()\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, outputs, target):\n",
        "        assert(len(outputs) == len(target))\n",
        "        assert(len(self.loss_function) == len(target) \\\n",
        "            or len(self.loss_function) == 1)\n",
        "\n",
        "        # expand loss_function list if univariate\n",
        "        if len(self.loss_function) == 1:\n",
        "            self.loss_function = [self.loss_function[0] for i in range(len(target))]\n",
        "        # expand weights list if univariate\n",
        "        if len(self.weights) == 1:\n",
        "            self.weights = [self.weights[0] for i in range(len(target))]\n",
        "\n",
        "        # compute loss for each head\n",
        "        total_loss = 0.\n",
        "        for out, gt, loss_func, weight in zip(outputs, target, self.loss_function, self.weights):\n",
        "            loss = loss_func(out, gt)\n",
        "            total_loss += loss * weight\n",
        "        return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOmPtnqwMo2"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NunwLfQVwO6I"
      },
      "source": [
        "from sklearn.metrics import recall_score, roc_curve, auc\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    return recall_score(y_true, y_pred, pos_label=0)\n",
        "\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    return recall_score(y_true, y_pred, pos_label=1)\n",
        "\n",
        "\n",
        "def balanced_accuracy(y_true, y_pred):\n",
        "    spec = specificity(y_true, y_pred)\n",
        "    sens = sensitivity(y_true, y_pred)\n",
        "    return (spec + sens) / 2\n",
        "\n",
        "def auc_score(y_true, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    return auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSFMH4TjzEjx"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_0C35vEwWW1"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# nitorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YTI5UdOzMz0"
      },
      "source": [
        "class Callback:\n",
        "    \"\"\"\n",
        "    Abstract class for callbacks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def final(self, **kwargs):\n",
        "        self.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF-4GFwuzTBw"
      },
      "source": [
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    Arguments:\n",
        "        path:\n",
        "        num_iters: number of iterations after which to store the model.\n",
        "            If set to -1, it will only store the last iteration's model.\n",
        "        prepend: string to prepend the filename with.\n",
        "        ignore_before: ignore early iterations.\n",
        "        store_best: boolen whether to save the best model during\n",
        "            training.\n",
        "        store_best_metric: name of the metric to use for best model\n",
        "            selection.\n",
        "        mode: \"max\" or \"min\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path,\n",
        "        retain_metric=\"loss\",\n",
        "        prepend=\"\",\n",
        "        num_iters=-1,\n",
        "        ignore_before=0,\n",
        "        store_best=False,\n",
        "        mode=\"max\"\n",
        "        ):\n",
        "        super().__init__()\n",
        "        if os.path.isdir(path):\n",
        "            self.path = path\n",
        "        else:\n",
        "            os.makedirs(path)\n",
        "            self.path = path\n",
        "        # end the prepended text with an underscore if it does not\n",
        "        if not prepend.endswith(\"_\") and prepend != \"\":\n",
        "            prepend += \"_\"\n",
        "        self.prepend = prepend\n",
        "        self.num_iters = num_iters\n",
        "        self.ignore_before = ignore_before\n",
        "        self.best_model = None\n",
        "        self.best_res = -1\n",
        "        self.store_best = store_best\n",
        "        self.retain_metric = retain_metric\n",
        "        self.mode = mode\n",
        "\n",
        "        print(self.path)\n",
        "\n",
        "    def __call__(self, trainer, epoch, val_metrics):\n",
        "        # do not store intermediate iterations\n",
        "        print(self.path)\n",
        "        if epoch >= self.ignore_before and epoch != 0:\n",
        "            if not self.num_iters == -1:\n",
        "\n",
        "                # counting epochs starts from 1; i.e. +1\n",
        "                epoch += 1\n",
        "                # store model recurrently if set\n",
        "                if epoch % self.num_iters == 0:\n",
        "                    name = self.prepend + \"training_epoch_{}.h5\".format(epoch)\n",
        "                    full_path = os.path.join(self.path, name)\n",
        "                    self.save_model(trainer, full_path)\n",
        "\n",
        "            # store current model if improvement detected\n",
        "            if self.store_best:\n",
        "                current_res = 0\n",
        "                try:\n",
        "                    # check if value can be used directly or not\n",
        "                    if isinstance(self.retain_metric, str):\n",
        "                        current_res = val_metrics[self.retain_metric][-1]\n",
        "                    else:\n",
        "                        current_res = val_metrics[self.retain_metric.__name__][-1]\n",
        "                except KeyError:\n",
        "                    print(\"Couldn't find {} in validation metrics. Using \\\n",
        "                        loss instead.\".format(self.retain_metric))\n",
        "                    current_res = val_metrics[\"loss\"][-1]\n",
        "\n",
        "                if self.has_improved(current_res):\n",
        "                    self.best_res = current_res\n",
        "                    self.best_model = deepcopy(trainer.model.state_dict())\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset module after training.\n",
        "        Useful for cross validation.\n",
        "        \"\"\"\n",
        "        self.best_model = None\n",
        "        self.best_res = -1\n",
        "\n",
        "    def final(self, **kwargs):\n",
        "        epoch = kwargs[\"epoch\"] + 1\n",
        "        if epoch >= self.ignore_before:\n",
        "            name = self.prepend + \"training_epoch_{}_FINAL.h5\".format(epoch)\n",
        "            full_path = os.path.join(self.path, name)\n",
        "            self.save_model(kwargs[\"trainer\"], full_path)\n",
        "        else:\n",
        "            print(\"Minimum iterations to store model not reached.\")\n",
        "\n",
        "        if self.best_model is not None:\n",
        "            best_model = deepcopy(self.best_model)\n",
        "            best_res = self.best_res\n",
        "            print(\"Best result during training: {:.2f}. Saving model..\".format(best_res))\n",
        "            name = self.prepend + \"BEST_ITERATION.h5\"\n",
        "            torch.save(best_model, os.path.join(self.path, name))\n",
        "        self.reset()\n",
        "\n",
        "    def save_model(self, trainer, full_path):\n",
        "        print(\"Writing model to disk...\")\n",
        "        model = trainer.model.cpu()\n",
        "        torch.save(model.state_dict(), full_path)\n",
        "        if trainer.device is not None:\n",
        "            trainer.model.cuda(trainer.device)\n",
        "\n",
        "    def has_improved(self, res):\n",
        "        if self.mode == \"max\":\n",
        "            return res >= self.best_res\n",
        "        elif self.mode == \"min\":\n",
        "            # check if still standard value\n",
        "            if self.best_res == -1:\n",
        "                return True\n",
        "            else:\n",
        "                return res <= self.best_res\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only modes 'min' and 'max' available\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D6jaW-OzcXE"
      },
      "source": [
        "class EarlyStopping(Callback):\n",
        "    \"\"\"\n",
        "    Stop training when a monitored quantity has stopped improving.\n",
        "    Arguments\n",
        "        patience: number of iterations without improvement after which\n",
        "            to stop\n",
        "        retain_metric: the metric which you want to monitor\n",
        "        mode: {min or max}; defines if you want to maximise or minimise\n",
        "            your metric\n",
        "        ignore_before: does not start the first window until this epoch.\n",
        "            Can be useful when training spikes a lot in early epochs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, patience, retain_metric, mode, ignore_before=0):\n",
        "        self.patience = patience\n",
        "        self.retain_metric = retain_metric\n",
        "        self.mode = mode\n",
        "        self.ignore_before = ignore_before\n",
        "        self.best_res = -1\n",
        "        # set to first iteration which is interesting\n",
        "        self.best_epoch = self.ignore_before\n",
        "\n",
        "    def __call__(self, trainer, epoch, val_metrics):\n",
        "        if epoch >= self.ignore_before:\n",
        "            if epoch - self.best_epoch < self.patience:\n",
        "                if isinstance(self.retain_metric, str):\n",
        "                    current_res = val_metrics[self.retain_metric][-1]\n",
        "                else:\n",
        "                    current_res = val_metrics[self.retain_metric.__name__][-1]\n",
        "                if self.has_improved(current_res):\n",
        "                    self.best_res = current_res\n",
        "                    self.best_epoch = epoch\n",
        "            else:\n",
        "                # end training run\n",
        "                trainer.stop_training = True\n",
        "\n",
        "    def has_improved(self, res):\n",
        "        if self.mode == \"max\":\n",
        "            return res > self.best_res\n",
        "        elif self.mode == \"min\":\n",
        "            # check if still standard value\n",
        "            if self.best_res == -1:\n",
        "                return True\n",
        "            else:\n",
        "                return res < self.best_res\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only modes 'min' and 'max' available\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Resets after training. Useful for cross validation.\"\"\"\n",
        "        self.best_res = -1\n",
        "        self.best_epoch = self.ignore_before\n",
        "\n",
        "    def final(self, **kwargs):\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "# Functions which can be used in custom-callbacks for visualizing 3D-features during training\n",
        "# (using the argument 'training_time_callback' in nitorch's Trainer class )\n",
        "def visualize_feature_maps(features, return_fig=False):\n",
        "\n",
        "    if(features.is_cuda):\n",
        "        features = features.cpu().detach().numpy()\n",
        "\n",
        "    num_features = len(features)\n",
        "    plt.close('all')\n",
        "    figsize=((num_features//8 + 5)*3 ,(num_features//8)*10 )\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    for i, f in enumerate(features, 1):\n",
        "        # normalize to range [0, 1] first as the values can be very small\n",
        "        if((f.max() - f.min()) != 0):\n",
        "            f = (f - f.min()) / (f.max() - f.min())\n",
        "\n",
        "            idxs = np.nonzero(f)\n",
        "            vals = np.ravel(f[idxs])\n",
        "            if(len(vals)):\n",
        "                # calculate the index where the mean value would lie\n",
        "                mean_idx = np.average(idxs, axis = 1, weights=vals)\n",
        "                # calculate the angel ratios for each non-zero val\n",
        "                angles = (mean_idx.reshape(-1,1) - idxs)\n",
        "                angles = angles/ (np.max(abs(angles), axis=1).reshape(-1,1))\n",
        "            else: # if all values in f are zero, set dummy angle\n",
        "                angles = [1, 1, 1]\n",
        "\n",
        "#             print(\"values = \",vals)\n",
        "            ax = fig.add_subplot(num_features//3+1, 3, i,\n",
        "                                  projection='3d')\n",
        "            ax.set_title(\"Feature-{} in the bottleneck\".format(i))\n",
        "            ax.quiver(*idxs\n",
        "                      , angles[0]*vals, angles[1]*vals, angles[2]*vals\n",
        "                     )\n",
        "            plt.grid()\n",
        "\n",
        "        else:\n",
        "            ax = fig.add_subplot(num_features//3+1, 3, i)\n",
        "            ax.text(0.5, 0.5, \"All values zero!\", transform=ax.transAxes)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if return_fig:\n",
        "        return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ1jrMnUzjeA"
      },
      "source": [
        "class CAE_VisualizeTraining(Callback):\n",
        "    '''\n",
        "    training_time_callback that prints the model dimensions,\n",
        "    visualizes CAE encoder outputs, original image and reconstructed image\n",
        "    during training.\n",
        "\n",
        "    NOTE : The forward() function of the CAE model using this callback\n",
        "    must return a (decoder_output, encoder_output) tuple.\n",
        "    '''\n",
        "    def __init__(self, model, max_train_iters, show_epochs_list=[], plotFeatures=True, plot_pdf_path=\"\", cmap=\"nipy_spectral\"):\n",
        "        self.model = model\n",
        "        self.max_train_iters = max_train_iters\n",
        "        if plot_pdf_path is not None:\n",
        "            assert isinstance(plot_pdf_path, str), \"pp is not a path!\"\n",
        "        self.plot_pdf_path = plot_pdf_path\n",
        "        assert isinstance(plotFeatures, bool), \"plotFeatures not boolean object!\"\n",
        "        self.plotFeatures = plotFeatures\n",
        "        assert isinstance(show_epochs_list, list), \"show_epochs_list is not a list!\"\n",
        "        self.show_epochs_list = show_epochs_list\n",
        "        self.cmap = cmap\n",
        "        self.ave_grads = []\n",
        "        self.layers = []\n",
        "        # inform the model to also return the encoder output along with the decoder output\n",
        "        try:\n",
        "            if(isinstance(model, nn.DataParallel)):\n",
        "                model.module.set_return_encoder_out(True)\n",
        "            else:\n",
        "                model.set_return_encoder_out(True)\n",
        "        except AttributeError:\n",
        "            raise \"The CAE model must implement a setter function 'set_return_encoder_out'\\\n",
        " for a flag 'encoder_out' which when set to true, the forward() function using this callback \\\n",
        "must return a (decoder_output, encoder_output) tuple instead of just (encoder_output). See the CAE class in models.py for the framework.\"\n",
        "\n",
        "    def __call__(self, inputs, labels, train_iter, epoch):\n",
        "        debug = False\n",
        "        visualize_training = False\n",
        "        tmp_show_epoches_list = []\n",
        "\n",
        "        # if show_epochs_list is empty, all epoches should be plotted. Therefore, add current epoch to the list\n",
        "        if not self.show_epochs_list:\n",
        "            tmp_show_epoches_list.append(epoch)\n",
        "        else:\n",
        "            tmp_show_epoches_list = self.show_epochs_list\n",
        "\n",
        "        # check if epoch should be visualized\n",
        "        if epoch in tmp_show_epoches_list:\n",
        "            # print the model's parameter dimensions etc in the first iter\n",
        "            if (train_iter == 0 and epoch == 0):\n",
        "                debug = True\n",
        "            # visualize training on the last iteration in that epoch\n",
        "            elif(train_iter==1 and epoch==0) or (train_iter == self.max_train_iters):\n",
        "                visualize_training = True\n",
        "\n",
        "        # for nitorch models which have a 'debug' and 'visualize_training' switch in the\n",
        "        # forward() method\n",
        "\n",
        "        if(isinstance(self.model, nn.DataParallel)):\n",
        "            self.model.module.set_debug(debug)\n",
        "        else:\n",
        "            self.model.set_debug(debug)\n",
        "\n",
        "        outputs, encoder_out = self.model(inputs)\n",
        "\n",
        "        if(visualize_training):\n",
        "            # check if result should be plotted in PDF\n",
        "            if self.plot_pdf_path != \"\":\n",
        "                pp = PdfPages(os.path.join(self.plot_pdf_path, \"training_epoch_\" + str(epoch) + \"_visualization.pdf\"))\n",
        "            else:\n",
        "                pp = None\n",
        "\n",
        "            # show only the first image in the batch\n",
        "            if pp is None:\n",
        "                # input image\n",
        "                show_brain(inputs[0].squeeze().cpu().detach().numpy(),  draw_cross=False, cmap=self.cmap)\n",
        "                plt.suptitle(\"Input image\")\n",
        "                plt.show()\n",
        "                if(not torch.all(torch.eq(inputs[0],labels[0]))):\n",
        "                    show_brain(labels[0].squeeze().cpu().detach().numpy(),  draw_cross = False, cmap=self.cmap)\n",
        "                    plt.suptitle(\"Expected reconstruction\")\n",
        "                    plt.show()\n",
        "                # reconstructed image\n",
        "                show_brain(outputs[0].squeeze().cpu().detach().numpy(),  draw_cross = False, cmap=self.cmap)\n",
        "                plt.suptitle(\"Reconstructed Image\")\n",
        "                plt.show()\n",
        "                # statistics\n",
        "                print(\"\\nStatistics of expected reconstruction:\\n(min, max)=({:.4f}, {:.4f})\\nmean={:.4f}\\nstd={:.4f}\".format(\n",
        "                    labels[0].min(), labels[0].max(), labels[0].mean(), labels[0].std()))\n",
        "                print(\"\\nStatistics of Reconstructed image:\\n(min, max)=({:.4f}, {:.4f})\\nmean={:.4f}\\nstd={:.4f}\".format(\n",
        "                    outputs[0].min(), outputs[0].max(), outputs[0].mean(), outputs[0].std()))\n",
        "                # feature maps\n",
        "                visualize_feature_maps(encoder_out[0])\n",
        "                plt.suptitle(\"Encoder output\")\n",
        "                plt.show()\n",
        "            else:\n",
        "                # input image\n",
        "                fig = show_brain(inputs[0].squeeze().cpu().detach().numpy(),  draw_cross=False, return_fig=True,\n",
        "                                 cmap=self.cmap)\n",
        "                plt.suptitle(\"Input image\")\n",
        "                pp.savefig(fig)\n",
        "                plt.close(fig)\n",
        "                if(not torch.all(torch.eq(inputs[0],labels[0]))):\n",
        "                    fig = show_brain(labels[0].squeeze().cpu().detach().numpy(),  draw_cross = False, cmap=self.cmap)\n",
        "                    plt.suptitle(\"Expected reconstruction\")\n",
        "                    pp.savefig(fig)\n",
        "                    plt.close(fig)\n",
        "                # reconstructed image\n",
        "                fig = show_brain(outputs[0].squeeze().cpu().detach().numpy(), draw_cross=False, return_fig=True, cmap=self.cmap)\n",
        "                plt.suptitle(\"Reconstructed Image\")\n",
        "                pp.savefig(fig)\n",
        "                plt.close(fig)\n",
        "                # feature maps\n",
        "                if self.plotFeatures:\n",
        "                    fig = visualize_feature_maps(encoder_out[0], return_fig=True)\n",
        "                    plt.suptitle(\"Encoder output\")\n",
        "                    pp.savefig(fig)\n",
        "                    plt.close(fig)\n",
        "\n",
        "            # close the PDF\n",
        "            if pp is not None:\n",
        "                pp.close()\n",
        "\n",
        "        if(isinstance(self.model, nn.DataParallel)):\n",
        "            self.model.module.set_debug(False)\n",
        "        else:\n",
        "            self.model.set_debug(False)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3jCk_JX1kg7"
      },
      "source": [
        "# ADNI Training (AD vs CN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs-pXXkX1r91"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZudvU3LW2A4n"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap_02Er01yy9"
      },
      "source": [
        "import glob\n",
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht3ikaW3DcxF",
        "outputId": "4faa409b-51d2-41a4-93d8-bfa462ffad07"
      },
      "source": [
        "!pip install nilearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.4.1)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.19.5)\n",
            "Requirement already satisfied: nibabel>=2.5 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->nilearn) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->nilearn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TtQu5XoB2vE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d571ba68-84fb-4d06-e050-c5e625786165"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import nilearn as nil\n",
        "from nilearn import plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "# ensure directory structure includes 'nitorch' from https://github.com/moboehle/Pytorch-LRP/tree/master/nitorch,\n",
        "# as there are other packages named 'nitorch' that will not work here\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
            "  \"Numpy arrays.\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qn-4oCKEGkB"
      },
      "source": [
        "## Read in data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = ('/content/drive/MyDrive/ADNI_whole/ADvsCN/train/') # change this to directory with MRI images\n",
        "img_files = sorted(os.listdir(img_dir))\n",
        "\n",
        "\n",
        "# read in CSV description of downloaded MRI scans\n",
        "df = pd.read_csv('/content/drive/MyDrive/ADNI_whole/ADvsCN/AD_CN_3.csv')\n",
        "\n",
        "# create x and y arrays\n",
        "group = []\n",
        "for i in range(len(img_files)):\n",
        "    idx = img_files[i][5:15]\n",
        "    group.append(df.loc[df.Subject==idx,'Group'].values[0])\n",
        "group = ((np.array(group) == 'AD') / 1)\n",
        "\n",
        "tmp = np.arange(len(img_files))\n",
        "train, val = train_test_split(tmp, test_size=.1, shuffle = True, stratify = group, random_state = 12)\n",
        "x_train = []\n",
        "for file in np.array(img_files)[train]:\n",
        "    x_train.append(np.array([nib.load(img_dir + file).get_fdata()[:,72:152:8,:,:]]))\n",
        "x_train = np.array(x_train)[:,0,:,:,:,:]\n",
        "y_train = group[train]\n",
        "x_val = np.array([nib.load(img_dir + file).get_fdata()[:,72:152:8,:,:] for file in np.array(img_files)[val]])\n",
        "y_val = group[val]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(sum(y_train))\n",
        "print(sum(y_val))"
      ],
      "metadata": {
        "id": "2QEtdbCEf9bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haxi9TrSGMOo"
      },
      "source": [
        "def show_slices(image):\n",
        "    fig, axes = plt.subplots(1, 10, figsize = (20, 20))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(image[:, i, :].T, cmap=\"gray\", origin=\"lower\")\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title('Slice {}'.format(72 + 8 * i))\n",
        "\n",
        "show_slices(x_train[0,:,:,:,0])\n",
        "show_slices(x_train[50,:,:,:,0])\n",
        "show_slices(x_train[200,:,:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuP7xttspgvG"
      },
      "source": [
        "## Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BAjY7djGXKj"
      },
      "source": [
        "def crop(x, z_slice=0, dims=(224, 224)):\n",
        "    x = np.moveaxis(x, 2, 3)\n",
        "    x = np.moveaxis(x[:,:,:,z_slice], -1, 1)\n",
        "    x_new = np.zeros((x.shape[0], x.shape[1], dims[0], dims[1]))\n",
        "    for i in range(len(x)):\n",
        "        data = np.argwhere(x[i][0])\n",
        "        min_idx = np.min(data, axis=0)\n",
        "        max_idx = np.max(data, axis=0)\n",
        "        crop = x[i][0][min_idx[0]:max_idx[0], min_idx[1]:max_idx[1]]\n",
        "        left_right = int((dims[0]-crop.shape[0])/2)\n",
        "        bottom_top = int((dims[1]-crop.shape[1])/2)\n",
        "        x_new[i][0] = np.pad(crop, ((left_right, dims[0]-crop.shape[0]-left_right), (dims[1]-crop.shape[1]-bottom_top, bottom_top)))\n",
        "\n",
        "    return x_new\n",
        "\n",
        "def min_max(x):\n",
        "    for i in range(len(x)):\n",
        "        x[i] -= np.min(x[i])\n",
        "        x[i] /= np.max(x[i])\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_h5zms2TyY"
      },
      "source": [
        "## Resnet with bottleneck Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhwjdm-4pJTz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "\n",
        "\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, n_dims, width=14, height=14, heads=4):\n",
        "        super(MHSA, self).__init__()\n",
        "        self.heads = heads\n",
        "\n",
        "        self.query = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "        self.key = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "        self.value = nn.Conv2d(n_dims, n_dims, kernel_size=1)\n",
        "\n",
        "        self.rel_h = nn.Parameter(torch.randn([1, heads, n_dims // heads, 1, height]), requires_grad=True)\n",
        "        self.rel_w = nn.Parameter(torch.randn([1, heads, n_dims // heads, width, 1]), requires_grad=True)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_batch, C, width, height = x.size()\n",
        "        q = self.query(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "        k = self.key(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "        v = self.value(x).view(n_batch, self.heads, C // self.heads, -1)\n",
        "\n",
        "        content_content = torch.matmul(q.permute(0, 1, 3, 2), k)\n",
        "\n",
        "        content_position = (self.rel_h + self.rel_w).view(1, self.heads, C // self.heads, -1).permute(0, 1, 3, 2)\n",
        "        content_position = torch.matmul(content_position, q)\n",
        "\n",
        "        energy = content_content + content_position\n",
        "        attention = self.softmax(energy)\n",
        "\n",
        "        out = torch.matmul(v, attention.permute(0, 1, 3, 2))\n",
        "        out = out.view(n_batch, C, width, height)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, heads=4, mhsa=False, resolution=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        if not mhsa:\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        else:\n",
        "            self.conv2 = nn.ModuleList()\n",
        "            self.conv2.append(MHSA(planes, width=int(resolution[0]), height=int(resolution[1]), heads=heads))\n",
        "            if stride == 2:\n",
        "                self.conv2.append(nn.AvgPool2d(2, 2))\n",
        "            self.conv2 = nn.Sequential(*self.conv2)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# reference\n",
        "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=2, resolution=(224, 224), heads=4):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.resolution = list(resolution)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        if self.conv1.stride[0] == 2:\n",
        "            self.resolution[0] /= 2\n",
        "        if self.conv1.stride[1] == 2:\n",
        "            self.resolution[1] /= 2\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # for ImageNet\n",
        "        if self.maxpool.stride == 2:\n",
        "            self.resolution[0] /= 2\n",
        "            self.resolution[1] /= 2\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, heads=heads, mhsa=True)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.3), # All architecture deeper than ResNet-200 dropout_rate: 0.2\n",
        "            nn.Linear(512 * block.expansion, num_classes)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1, heads=4, mhsa=False):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for idx, stride in enumerate(strides):\n",
        "            layers.append(block(self.in_planes, planes, stride, heads, mhsa, self.resolution))\n",
        "            if stride == 2:\n",
        "                self.resolution[0] /= 2\n",
        "                self.resolution[1] /= 2\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out) # for ImageNet\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb4ReIt9Gdvg"
      },
      "source": [
        "## Build an ensemble based on specified slices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA66Wdeoq7uc"
      },
      "source": [
        "gpu = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7bYxesTrAHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15156a86-13db-4c39-9b22-e12ea8830ee0"
      },
      "source": [
        "pip install sam-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sam-pytorch\n",
            "  Downloading sam_pytorch-0.0.1-py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sam-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->sam-pytorch) (3.10.0.2)\n",
            "Installing collected packages: sam-pytorch\n",
            "Successfully installed sam-pytorch-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-CGY-ArA-_"
      },
      "source": [
        "from sam import SAM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpSK7mq5GrPw"
      },
      "source": [
        "for z_slice in range(0, 7):\n",
        "    print('Starting slice {}'.format(z_slice))\n",
        "\n",
        "    x_train_z = crop(x_train, z_slice=z_slice, dims=(224, 224))\n",
        "    x_train_z = min_max(x_train_z)\n",
        "    x_val_z = crop(x_val, z_slice=z_slice, dims=(224, 224))\n",
        "    x_val_z = min_max(x_val_z)\n",
        "\n",
        "    print(x_train_z.shape)\n",
        "    print(x_val_z.shape)\n",
        "\n",
        "    # https://github.com/moboehle/Pytorch-LRP/blob/master/ADNI%20Training.ipynb\n",
        "    class ADNIDataset(Dataset):\n",
        "        def __init__(self, X, y, transform=None, target_transform=None, mask=None, z_factor=None, dtype=torch.float32, num_classes=2):\n",
        "            self.X = np.copy(X)\n",
        "            self.y = np.copy(y)\n",
        "            self.X = X\n",
        "            self.y = y\n",
        "            self.transform = transform\n",
        "            self.target_transform = target_transform\n",
        "            self.mask = mask\n",
        "            self.z_factor = z_factor\n",
        "            self.dtype = dtype\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image = self.X[idx]\n",
        "            label_tensor = np.zeros(shape=(self.num_classes,))\n",
        "            label = self.y[idx] >= 0.5\n",
        "            label = torch.LongTensor([label])\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            sample = {\"image\" : image,\n",
        "                     \"label\" : label}\n",
        "            return sample\n",
        "\n",
        "    augmentations = [SagittalRotate()]\n",
        "    adni_data_train = ADNIDataset(x_train_z[:,0,:,:], y_train, transform = transforms.Compose(augmentations + [ToTensor()]),\n",
        "                                  dtype=torch.float32)\n",
        "    adni_data_val = ADNIDataset(x_val_z[:,0,:,:], y_val, transform = transforms.Compose(augmentations + [ToTensor()]),\n",
        "                                  dtype=torch.float32)\n",
        "\n",
        "\n",
        "    net = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=2, resolution=(224, 224), heads=8).cuda(gpu)\n",
        "\n",
        "\n",
        "    def run(\n",
        "        net,\n",
        "        data,\n",
        "        shape,\n",
        "        callbacks=[],\n",
        "        augmentations=[],\n",
        "        masked=False,\n",
        "        metrics=[],\n",
        "        k_folds=None,\n",
        "        b=4,\n",
        "        num_epochs=60,\n",
        "        retain_metric=None):\n",
        "\n",
        "        fold_metric = []\n",
        "        models = []\n",
        "        fold = 0\n",
        "        initial_prepend = None\n",
        "\n",
        "        # set number of cross-validation folds\n",
        "        for trial in range(5):\n",
        "            print(\"Starting trial {}\".format(trial))\n",
        "\n",
        "            # add current fold number to model checkpoint path\n",
        "            if callbacks is not None:\n",
        "                for idx, callback in enumerate(callbacks):\n",
        "                    if isinstance(callback, ModelCheckpoint):\n",
        "                        if initial_prepend is None:\n",
        "                            initial_prepend = callbacks[idx].prepend\n",
        "                        callbacks[idx].prepend = initial_prepend + \"cv_fold_{}_\".format(fold)\n",
        "            fold += 1\n",
        "\n",
        "            # restart model\n",
        "            del net\n",
        "            net = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=2, resolution=(224, 224), heads=8).cuda(gpu)\n",
        "\n",
        "            # reset hyperparameters\n",
        "            lr = 3e-5\n",
        "            wd = 3e-5\n",
        "            criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
        "            base_optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)  # define an optimizer for the \"sharpness-aware\" update\n",
        "            optimizer = SAM(net.parameters(), base_optimizer)\n",
        "\n",
        "            train_loader = DataLoader(adni_data_train, batch_size=b, num_workers=0, shuffle=True)\n",
        "            val_loader = DataLoader(adni_data_val, batch_size=1, num_workers=0, shuffle=True)\n",
        "\n",
        "\n",
        "            # plot slice (optional)\n",
        "            sample = next(iter(train_loader))\n",
        "            img = sample[\"image\"][0]\n",
        "            lbl = sample[\"label\"][0]\n",
        "            plt.imshow(img.squeeze(), cmap='gray')\n",
        "            plt.title(lbl.item())\n",
        "            plt.show()\n",
        "            model_trainer = Trainer(\n",
        "                net,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                metrics=metrics,\n",
        "                callbacks=None,\n",
        "                device=gpu,\n",
        "                prediction_type=\"classification\")\n",
        "\n",
        "            # train model and store results\n",
        "            net, report = model_trainer.train_model(\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                num_epochs=num_epochs,\n",
        "                show_train_steps=100,\n",
        "                show_validation_epochs=1)\n",
        "            # append validation score of the retain metric\n",
        "            if isinstance(retain_metric, str):\n",
        "                fold_metric.append(report[\"val_metrics\"][retain_metric][-1])\n",
        "            else:\n",
        "                fold_metric.append(report[\"val_metrics\"][retain_metric.__name__][-1])\n",
        "\n",
        "            models.append(net)\n",
        "            print(\"Finished fold.\")\n",
        "\n",
        "            # visualize result (optional)\n",
        "            model_trainer.visualize_training(report, metrics)\n",
        "            model_trainer.evaluate_model(val_loader)\n",
        "\n",
        "        print(\"################################\")\n",
        "        print(\"################################\")\n",
        "        print(\"All accuracies: {}\".format(fold_metric))\n",
        "        return fold_metric, models\n",
        "\n",
        "    net = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=2, resolution=(224, 224), heads=8).cuda(gpu)\n",
        "    #net = VisionTransformer().to('cuda')\n",
        "    num_epochs = 60\n",
        "    min_iters = 3\n",
        "    ignore_epochs = 15\n",
        "    normalize = False\n",
        "    retain_metric = accuracy_score\n",
        "    metrics = [accuracy_score]\n",
        "    model_path = '/content/drive/MyDrive/ADNI_whole/new_trained_ad_cn'\n",
        "    r = 0\n",
        "\n",
        "    check = ModelCheckpoint(path=model_path,\n",
        "                        prepend=\"repeat_{}\".format(r),\n",
        "                        store_best=True,\n",
        "                        ignore_before=ignore_epochs,\n",
        "                        retain_metric=retain_metric)\n",
        "\n",
        "    callbacks = [check, EarlyStopping(patience=10, ignore_before=ignore_epochs, retain_metric=\"loss\", mode='min')]\n",
        "\n",
        "    fold_metric, models = run(net=net, data=adni_data_train,\n",
        "                          k_folds=-1,\n",
        "                          callbacks=None,\n",
        "                          shape=-1,\n",
        "                          masked=False,\n",
        "                          metrics=metrics,\n",
        "                          b=4,\n",
        "                          num_epochs=num_epochs,\n",
        "                          retain_metric=retain_metric\n",
        "                          )\n",
        "\n",
        "    print(np.mean(fold_metric))\n",
        "    print(np.std(fold_metric))\n",
        "    best = np.array(fold_metric).argmax()\n",
        "    print(best)\n",
        "    torch.save(models[best].state_dict(), model_path + '/' + '2D_Slice_' + str(z_slice))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU8NGsL9uqrV"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h5nPSfhutUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16b5aed-9c4c-45c1-84b6-6eeafcb26b47"
      },
      "source": [
        "img_dir = ('/content/drive/MyDrive/ADNI_whole/ADvsCN/test/') # change this to directory with MRI images\n",
        "img_files = sorted(os.listdir(img_dir))\n",
        "\n",
        "\n",
        "# read in CSV description of downloaded MRI scans\n",
        "df = pd.read_csv('/content/drive/MyDrive/ADNI_whole/ADvsCN/AD_CN_3.csv')\n",
        "\n",
        "# create x and y arrays\n",
        "group = []\n",
        "for i in range(len(img_files)):\n",
        "    idx = img_files[i][5:15]\n",
        "    group.append(df.loc[df.Subject==idx,'Group'].values[0])\n",
        "group = ((np.array(group) == 'CN') / 1)\n",
        "ims = np.array([nib.load(img_dir + file).get_fdata()[:,:,30:70:4] for file in img_files])\n",
        "\n",
        "tmp = np.arange(len(img_files))\n",
        "#train, val = train_test_split(tmp, test_size=.1, shuffle = True, stratify = group, random_state = 12)\n",
        "x_test = []\n",
        "for file in np.array(img_files)[tmp]:\n",
        "    x_test.append(np.array([nib.load(img_dir + file).get_fdata()[:,72:152:8,:,:]]))\n",
        "x_test = np.array(x_test)[:,0,:,:,:,:]\n",
        "y_test = group[tmp]\n",
        "#x_val = np.array([nib.load(img_dir + file).get_fdata()[:,72:152:8,:,:] for file in np.array(img_files)[val]])\n",
        "#y_val = group[val]\n",
        "\n",
        "print(x_test.shape)\n",
        "#print(x_val.shape)\n",
        "print(y_test.shape)\n",
        "#print(y_val.shape)\n",
        "print(sum(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(71, 256, 10, 256, 1)\n",
            "(71,)\n",
            "38.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLGbdqZdu1tZ"
      },
      "source": [
        "class ADNIDataset(Dataset):\n",
        "  def __init__(self, X, y, transform=None, target_transform=None, mask=None, z_factor=None, dtype=torch.float32, num_classes=2):\n",
        "    self.X = np.copy(X)\n",
        "    self.y = np.copy(y)\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.mask = mask\n",
        "    self.z_factor = z_factor\n",
        "    self.dtype = dtype\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = self.X[idx]\n",
        "    label_tensor = np.zeros(shape=(self.num_classes,))\n",
        "    label = self.y[idx] >= 0.5\n",
        "    label = torch.LongTensor([label])\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    sample = {\"image\" : image,\n",
        "              \"label\" : label}\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "kDL7SrNWJfaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YszyJOiu6kp"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/ADNI_whole/trained_ad_cn'\n",
        "\n",
        "for z_slice in range(0, 7):\n",
        "  print('Starting slice {}'.format(z_slice))\n",
        "  y_hat = []\n",
        "  x_test_z = crop(x_test, z_slice=z_slice, dims=(224, 224))\n",
        "  x_test_z = min_max(x_test_z)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for _ in range(len(os.listdir(model_path))):\n",
        "      if type(ims) != torch.Tensor:\n",
        "        ims = torch.from_numpy(ims).float()\n",
        "      #x_test = torch.moveaxis(ims[:,:,:,_], -1, 1)\n",
        "     # for i in range(len(x_test)):\n",
        "      #  x_test[i] -= torch.min(x_test[i])\n",
        "       # x_test[i] /= torch.max(x_test[i])\n",
        "      #test_loader = DataLoader(ADNIDataset(x_test, group), batch_size=1, num_workers=0, shuffle=False)\n",
        "      augmentations = [SagittalRotate()]\n",
        "      adni_data_test = ADNIDataset(x_test_z[:,0,:,:], y_test, transform = transforms.Compose(augmentations + [ToTensor()]),\n",
        "                                  dtype=torch.float32)\n",
        "      test_loader = DataLoader(adni_data_test, batch_size=1, num_workers=0, shuffle=False)\n",
        "      all_preds = []\n",
        "      all_labels = []\n",
        "      model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=2, resolution=(224, 224), heads=8).cuda(gpu)\n",
        "      model.load_state_dict(torch.load(model_path + '/' + os.listdir(model_path)[_]))\n",
        "      model.eval()\n",
        "      for sample in test_loader:\n",
        "        img = sample[\"image\"]\n",
        "        label = sample[\"label\"]\n",
        "        img = img.to(torch.device(gpu))\n",
        "        output = model.forward(img)\n",
        "        #pred = torch.argmax(F.softmax(output, dim=1))\n",
        "        pred = F.softmax(output, dim=1)[0][1]\n",
        "        all_preds.append(pred.cpu().numpy().item())\n",
        "        all_labels.append(label.numpy().item())\n",
        "      print(all_preds)\n",
        "      print(all_labels)\n",
        "      #print(classification_report(all_labels, all_preds))\n",
        "      y_hat.append(all_preds)\n",
        "      del model\n",
        "  #y_test = group.astype(int)\n",
        "  y_hat = ((np.mean(y_hat, axis = 0) >= .4)/1).astype(int)\n",
        "  print(y_test)\n",
        "  print(y_hat)\n",
        "\n",
        "  print(classification_report(y_test, y_hat))\n",
        "  print(f'ROC_AUC score is {roc_auc_score(y_test, y_hat)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}